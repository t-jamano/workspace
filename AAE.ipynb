{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "from Models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import sentencepiece as spm\n",
    "\n",
    "path = \"/work/\"\n",
    "train_data = \"100K_QD_ml15\"\n",
    "\n",
    "max_len = 15\n",
    "enablePadding = True\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/work/data/bpe/en.wiki.bpe.op50000.model')\n",
    "bpe = KeyedVectors.load_word2vec_format(\"/work/data/bpe/en.wiki.bpe.op50000.d200.w2v.bin\", binary=True)\n",
    "bpe.index2word = [''] + bpe.index2word + ['<sos>'] + ['<eos>']  # add empty string \n",
    "nb_words = len(bpe.index2word)\n",
    "# word2index\n",
    "bpe_dict = {bpe.index2word[i]: i for i in range(len(bpe.index2word))}\n",
    "# construct embedding_matrix\n",
    "embedding_matrix = np.concatenate([np.zeros((1, bpe.vector_size)), bpe.vectors, np.zeros((2, bpe.vector_size))]) # add zero vector for empty string (i.e. used for padding)\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "                    embedding_matrix.shape[-1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=True)\n",
    "\n",
    "# test_set = []\n",
    "# for i in [\"MayFlower\", \"JuneFlower\", \"JulyFlower\", \"sts\", \"quora\", \"para\"]:\n",
    "#     df, qrel = get_test_data(i, path)\n",
    "#     q_ = parse_texts_bpe(df.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "#     d_ = parse_texts_bpe(df.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "#     test_set.append([q_, d_, qrel, df, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Models import *\n",
    "\n",
    "class VariationalAutoEncoder():\n",
    "\n",
    "    def __init__(self, nb_words, max_len, embedding_matrix, dim, optimizer=Adam(), kl_rate=0.01, keep_rate_word_dropout=0.5, enableKL=True, enableCond=False):\n",
    "\n",
    "        self.dim = dim\n",
    "        self.nb_words = nb_words\n",
    "        self.max_len = max_len\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.optimizer = optimizer\n",
    "        self.keep_rate_word_dropout = keep_rate_word_dropout\n",
    "        self.kl_rate = kl_rate\n",
    "        self.enableKL = enableKL\n",
    "        self.enableCond = enableCond\n",
    "\n",
    "        self.hidden_dim = self.dim[0]\n",
    "        self.latent_dim = self.dim[1]\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        query_inputs = Input(shape=(self.max_len,))\n",
    "        doc_inputs = Input(shape=(self.max_len,))\n",
    "        label_inputs = Input(shape=(1,))\n",
    "        kl_inputs = Input(shape=(1,))\n",
    "\n",
    "        encoder_embedding = Embedding(self.nb_words,\n",
    "                                        self.embedding_matrix.shape[-1],\n",
    "                                        weights=[self.embedding_matrix],\n",
    "                                        input_length=self.max_len,\n",
    "                                        mask_zero=True,\n",
    "                                        name=\"q_embeding_layer\",\n",
    "                                        trainable=True)\n",
    "\n",
    "        doc_encoder_embedding = Embedding(self.nb_words,\n",
    "                                        self.embedding_matrix.shape[-1],\n",
    "                                        weights=[self.embedding_matrix],\n",
    "                                        input_length=self.max_len,\n",
    "                                        mask_zero=True,\n",
    "                                        trainable=True)\n",
    "\n",
    "        norm = BatchNormalization()\n",
    "\n",
    "\n",
    "        encoder_lstm = GRU(self.hidden_dim, name=\"q_gru\")\n",
    "        doc_encoder_lstm = GRU(self.hidden_dim)\n",
    "\n",
    "\n",
    "        state = norm(encoder_lstm(encoder_embedding(query_inputs)))\n",
    "        doc_state = norm(doc_encoder_lstm(doc_encoder_embedding(doc_inputs)))\n",
    "\n",
    "        if self.enableCond:\n",
    "            state_z = merge([state, doc_state, label_inputs], mode=\"concat\")\n",
    "\n",
    "        state_z = state\n",
    "        self.mean = Dense(self.latent_dim)\n",
    "        self.var = Dense(self.latent_dim)\n",
    "\n",
    "        self.state_mean = self.mean(state_z)\n",
    "        self.state_var = self.var(state_z)\n",
    "\n",
    "\n",
    "\n",
    "        state_z = Lambda(self.sampling, name=\"kl\")([self.state_mean, self.state_var])\n",
    "\n",
    "        if self.enableCond:\n",
    "            state_z = merge([state_z, doc_state, label_inputs], mode=\"concat\")\n",
    "\n",
    "        decoder_inputs = Input(shape=(self.max_len,))\n",
    "\n",
    "        self.latent2hidden = Dense(self.hidden_dim)\n",
    "        self.decoder_lstm = GRU(self.hidden_dim, return_sequences=True, name=\"dec_gru\")\n",
    "        self.decoder_dense = Dense(self.nb_words, activation='softmax', name=\"rec\")\n",
    "        # self.decoder_embedding = Embedding(self.nb_words,\n",
    "        #                                 self.embedding_matrix.shape[-1],\n",
    "        #                                 weights=[self.embedding_matrix],\n",
    "        #                                 input_length=self.max_len,\n",
    "        #                                 mask_zero=True,\n",
    "        #                                 trainable=True)\n",
    "\n",
    "        rec_outputs = self.decoder_dense(self.decoder_lstm(encoder_embedding(decoder_inputs) , initial_state=self.latent2hidden(state_z)))\n",
    "\n",
    "        if self.enableKL:\n",
    "\n",
    "            def kl_annealing_loss(x, x_):\n",
    "                kl_loss = - 0.5 * K.sum(1 + state_var - K.square(state_mean) - K.exp(state_var), axis=-1)\n",
    "                return kl_inputs * kl_loss\n",
    "\n",
    "            self.model = Model([query_inputs, decoder_inputs, kl_inputs], [rec_outputs, state_z])\n",
    "            self.model.compile(optimizer=self.optimizer, loss=['sparse_categorical_crossentropy', kl_annealing_loss])\n",
    "\n",
    "        else:\n",
    "            if self.enableCond:\n",
    "                inputs = [query_inputs, decoder_inputs, doc_inputs, label_inputs]\n",
    "            else:\n",
    "                inputs = [query_inputs, decoder_inputs]\n",
    "            self.model = Model(inputs, [rec_outputs])\n",
    "            self.model.compile(optimizer=self.optimizer, loss=self.vae_loss, metrics=[self.rec_loss, self.kl_loss])        \n",
    "\n",
    "        self.encoder = Model(query_inputs, state)\n",
    "\n",
    "    # def vae_loss(self, y_true, y_pred):\n",
    "    # \trecon = K.sum(K.sparse_categorical_crossentropy(y_true, y_pred), axis=-1)\n",
    "    # \tkl = 0.5 * K.sum(K.exp(self.state_var) + K.square(self.state_mean) - 1. - self.state_var, axis=-1)\n",
    "    # \treturn recon + kl\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_onehot):\n",
    "        xent_loss = objectives.sparse_categorical_crossentropy(x, x_decoded_onehot)\n",
    "        kl_loss = - 0.5 * K.mean(1 + self.state_var - K.square(self.state_mean) - K.exp(self.state_var))\n",
    "        loss = xent_loss + kl_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def kl_loss(self, y_true, y_pred):\n",
    "        kl_loss = - 0.5 * K.mean(1 + self.state_var - K.square(self.state_mean) - K.exp(self.state_var))\n",
    "        return kl_loss\n",
    "\n",
    "    def rec_loss(self, y_true, y_pred):\n",
    "        return objectives.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    def name(self):\n",
    "        if self.enableCond:\n",
    "            return \"cvae\"\n",
    "        return \"vae\" if not self.enableKL else \"vae_kl\"\n",
    "\n",
    "    def word_dropout(self, x, unk_token):\n",
    "        np.random.seed(0)\n",
    "        x_ = np.copy(x)\n",
    "        rows, cols = np.nonzero(x_)\n",
    "        for r, c in zip(rows, cols):\n",
    "            if random.random() <= self.keep_rate_word_dropout:\n",
    "                continue\n",
    "            x_[r][c] = unk_token\n",
    "\n",
    "            return x_\n",
    "\n",
    "    def sampling(self, args):\n",
    "            z_mean, z_log_var = args\n",
    "            epsilon = K.random_normal(shape=(K.shape(z_mean)[0], K.shape(z_mean)[1]), mean=0.,\\\n",
    "                                      stddev=1)\n",
    "            return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Conv2DTranspose\n",
    "import keras.backend as K\n",
    "class Conv1DTranspose(Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=1, *args, **kwargs):\n",
    "        self._filters = filters\n",
    "        self._kernel_size = (1, kernel_size)\n",
    "        self._strides = (1, strides)\n",
    "        self._args, self._kwargs = args, kwargs\n",
    "        super(Conv1DTranspose, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._model = Sequential()\n",
    "        self._model.add(Lambda(lambda x: K.expand_dims(x,axis=1), batch_input_shape=input_shape))\n",
    "        self._model.add(Conv2DTranspose(self._filters,\n",
    "                                        kernel_size=self._kernel_size,\n",
    "                                        strides=self._strides,\n",
    "                                        *self._args, **self._kwargs))\n",
    "        self._model.add(Lambda(lambda x: x[:,0]))\n",
    "        super(Conv1DTranspose, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self._model.compute_output_shape(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# applying a more complex convolutional approach\n",
    "convs = []\n",
    "filter_sizes = [2,3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(15,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=25,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "x = Merge(mode='concat', concat_axis=1)(convs)\n",
    "\n",
    "# x = Conv1DTranspose(filters=128, kernel_size=1)(x)\n",
    "# softmax = Dense(5555, activation=\"softmax\")\n",
    "preds = x\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.random.randint(2, size=(5, 15))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
