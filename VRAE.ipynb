{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sentences from a Continuous Space\n",
    "### Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Models import *\n",
    "from Utils import *\n",
    "from FastModels import *\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 19898: expected 6 fields, saw 8\\nSkipping line 20620: expected 6 fields, saw 8\\nSkipping line 38039: expected 6 fields, saw 8\\n'\n"
     ]
    }
   ],
   "source": [
    "max_len = 10\n",
    "enablePadding = True\n",
    "nb_words= len(bpe_dict)\n",
    "\n",
    "df_may, qrel_may = get_test_data(\"MayFlower\", \"/work/\")\n",
    "df_june, qrel_june = get_test_data(\"JuneFlower\", \"/work/\")\n",
    "df_july, qrel_july = get_test_data(\"JulyFlower\", \"/work/\")\n",
    "\n",
    "q_may = parse_texts_bpe(df_may.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_may = parse_texts_bpe(df_may.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "q_june = parse_texts_bpe(df_june.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_june = parse_texts_bpe(df_june.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "q_july = parse_texts_bpe(df_july.q.tolist(), sp, bpe_dict, max_len, enablePadding, \"pre\")\n",
    "d_july = parse_texts_bpe(df_july.d.tolist(), sp, bpe_dict, max_len, enablePadding, \"pre\")\n",
    "\n",
    "test_set = [[q_may, d_may, qrel_may, df_may, \"MayFlower\"], [q_june, d_june, qrel_june, df_june, \"JuneFlower\"], [q_july, d_july, qrel_july, df_july, \"JulyFlower\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/work/data/bpe/en.wiki.bpe.op50000.model')\n",
    "bpe = KeyedVectors.load_word2vec_format(\"/work/data/bpe/en.wiki.bpe.op50000.d200.w2v.bin\", binary=True)\n",
    "bpe.index2word = [''] + bpe.index2word # add empty string\n",
    "nb_words = len(bpe.index2word)\n",
    "# word2index\n",
    "bpe_dict = {bpe.index2word[i]: i for i in range(len(bpe.index2word))}\n",
    "# construct embedding_matrix\n",
    "embedding_matrix = np.concatenate([np.zeros((1, bpe.vector_size)), bpe.vectors]) # add zero vector for empty string (i.e. used for padding)\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "                    embedding_matrix.shape[-1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/work/data/train_data/30M_QD_lower2.txt\", nrows=100000, names=[\"label\", \"q\", \"d\"], sep=\"\\t\", header=None, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_df = parse_texts_bpe(df.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "# np.save(\"/work/data/train_data/30M_QD_lower2.2.npy\", q_df)\n",
    "# x = np.load(\"/work/data/train_data/30M_QD_lower2.txt.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SeqVAE(object):\n",
    "    \n",
    "    def __init__(self, nb_words, max_len, emb, dim, optimizer=RMSprop(), word_dropout_prob=0.5, kl_weight=0):\n",
    "        self.dim = dim\n",
    "        self.nb_words = nb_words\n",
    "        self.max_len = max_len\n",
    "        self.emb = emb\n",
    "        self.optimizer = optimizer\n",
    "        self.kl_weight = kl_weight\n",
    "        self.word_dropout_prob = word_dropout_prob\n",
    "        \n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "                \n",
    "#       Encoder\n",
    "        \n",
    "        e_input = Input(shape=(self.max_len,))\n",
    "        e_mask = Masking(mask_value=0)\n",
    "        e_emb = self.emb\n",
    "        e_lstm = GRU(self.dim[0], return_state=True)\n",
    "        \n",
    "#         h, state_h, state_c = e_lstm(e_emb(e_mask(e_input)))#         \n",
    "        _, h = e_lstm(e_emb(e_mask(e_input)))\n",
    "\n",
    "        \n",
    "        mean = Dense(self.dim[1])\n",
    "        var = Dense(self.dim[1])\n",
    "        \n",
    "#         self.state_h_mean = mean(state_h)\n",
    "#         self.state_h_log_var = var(state_h)\n",
    "        \n",
    "#         self.state_c_mean = mean(state_c)\n",
    "#         self.state_c_log_var = var(state_c)\n",
    "        \n",
    "        self.h_mean = mean(h)\n",
    "        self.h_log_var = var(h)\n",
    "        \n",
    "#         state_h_z = Lambda(self.sampling)([self.state_h_mean, self.state_h_log_var])     \n",
    "#         state_c_z = Lambda(self.sampling)([self.state_c_mean, self.state_c_log_var])\n",
    "        z = Lambda(self.sampling)([self.h_mean, self.h_log_var])\n",
    "\n",
    "#         self.encoder = Model(e_input, self.state_h_mean)\n",
    "        self.encoder = Model(e_input, self.h_mean)\n",
    "        self.encoder2 = Model(e_input, z)\n",
    "\n",
    "        \n",
    "#       Decoder\n",
    "\n",
    "        d_input = Input(shape=(self.max_len,))\n",
    "        d_latent2hidden = Dense(self.dim[0])\n",
    "        d_lstm = GRU(self.dim[0], return_sequences=True)\n",
    "        d_output2vocab = TimeDistributed(Dense(self.nb_words, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "#         state_h_z = d_latent2hidden(state_h_z)\n",
    "#         state_c_z = d_latent2hidden(state_c_z)\n",
    "        h_z = d_latent2hidden(z)\n",
    "        \n",
    "        d_embed_input = e_emb(Dropout(self.word_dropout_prob)(d_input))\n",
    "#         d_embed_input = e_emb(d_input)\n",
    "\n",
    "#         outputs = d_lstm(d_embed_input, initial_state=[state_h_z, state_c_z])        \n",
    "        outputs = d_lstm(d_embed_input, initial_state=[h_z])\n",
    "\n",
    "        pred = d_output2vocab(outputs)\n",
    "\n",
    "        \n",
    "        \n",
    "#       VAE model\n",
    "        self.model = Model(inputs=[e_input, d_input], outputs=[pred, pred])\n",
    "        self.model.compile(optimizer=self.optimizer, loss=[\"sparse_categorical_crossentropy\", self.kl_loss])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def name(self):\n",
    "        \n",
    "        return \"seqvae\"\n",
    "    \n",
    "    def kl_loss(self, x, x_):\n",
    "#         x = K.flatten(x)\n",
    "#         x_ = K.flatten(x_)\n",
    "#         x = tf.cast(x, tf.int32)\n",
    "\n",
    "#         rec_loss = objectives.sparse_categorical_crossentropy(x,x_)\n",
    "\n",
    "#         state_h_kl_loss = - 0.5 * K.sum(1 + self.state_h_log_var - K.square(self.state_h_mean) - K.exp(self.state_h_log_var), axis=-1)\n",
    "#         state_c_kl_loss = - 0.5 * K.sum(1 + self.state_c_log_var - K.square(self.state_c_mean) - K.exp(self.state_c_log_var), axis=-1)\n",
    "        \n",
    "#         return (self.kl_weight * state_h_kl_loss) + (self.kl_weight * state_c_kl_loss)\n",
    "        kl_loss = - 0.5 * K.sum(1 + self.h_log_var - K.square(self.h_mean) - K.exp(self.h_log_var), axis=-1)\n",
    "        return (self.kl_weight * kl_loss) \n",
    "\n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.dim[1]), mean=0.,\\\n",
    "                                  stddev=1)\n",
    "    \n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon \n",
    "    \n",
    "    def nosampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        return z_mean + K.exp(z_log_var / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(epoch, logs):\n",
    "#     print(logs)\n",
    "    loss = logs.get(\"loss\")\n",
    "    val_loss = logs.get(\"val_loss\")\n",
    "    \n",
    "    q = run.encoder.predict(q_july)\n",
    "    d = run.encoder.predict(d_july)\n",
    "    \n",
    "    cosine = CosineSim(q.shape[-1])\n",
    "    pred = cosine.model.predict([q, d])\n",
    "    print(loss, val_loss, auc(qrel_july, pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KL_Annealing(Callback):\n",
    "    \n",
    "    def __init__(self, run):\n",
    "        super(KL_Annealing, self).__init__()\n",
    "        \n",
    "        self.run = run\n",
    "        self.kl_inc_rate = 1 / 5000. # set the annealing rate for KL loss\n",
    "        self.cos_inc_rate = 1\n",
    "        self.max_cos_weight = 150.\n",
    "        self.max_kl_weight = 1.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.run.kl_weight = min(self.run.kl_weight + self.kl_inc_rate, self.max_kl_weight)\n",
    "        self.run.model.compile(optimizer=self.run.optimizer, loss=[\"sparse_categorical_crossentropy\", self.run.kl_loss])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run = SeqVAE(nb_words, max_len, embedding_layer, [300, 200], optimizer=RMSprop(), word_dropout_prob=0.5, kl_weight=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train = [q_df, q_df]\n",
    "y_train = [np.expand_dims(q_df, axis=-1), np.expand_dims(q_df, axis=-1)]\n",
    "# tmp = np.concatenate([q_july, d_july])\n",
    "# x_train = [tmp, tmp]\n",
    "# y_train = [np.expand_dims(tmp, axis=-1), np.expand_dims(tmp, axis=-1)]\n",
    "# callbacks = [EarlyStopping(verbose=1, patience=5), ReduceLROnPlateau(verbose=1, patience=3),LambdaCallback(on_epoch_end=eval), TQDMNotebookCallback()]\n",
    "callbacks = [EarlyStopping(verbose=1, patience=5), ReduceLROnPlateau(verbose=1, patience=3), KL_Annealing(run), TQDMNotebookCallback()]\n",
    "# callbacks =[]\n",
    "# run.kl_weight = 0\n",
    "run.model.fit(x_train, y_train, validation_split=0.2, verbose=0, batch_size=256, epochs=50, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_output(model, bpe, x, nrows=None, idx=None):\n",
    "\n",
    "    gen_x = np.argmax(model.predict(x), axis=-1) if idx == None else np.argmax(model.predict([x,x])[idx], axis=-1)\n",
    "\n",
    "    bleu = []\n",
    "    results = \"\"\n",
    "    for i, k in zip(gen_x, x):\n",
    "        gen_x = \" \".join([bpe.index2word[t] for t in i])\n",
    "        real_x = \" \".join([bpe.index2word[t] for t in k])\n",
    "\n",
    "        bleu.append(sentence_bleu(real_x, gen_x))\n",
    "        \n",
    "        real_x = real_x.replace(\"▁the\", \"\")\n",
    "        real_x = real_x.replace(\"▁\",\"\")\n",
    "        gen_x = gen_x.replace(\"▁the\", \"\")\n",
    "        gen_x = gen_x.replace(\"▁\",\"\")\n",
    "        if nrows != None:\n",
    "            if nrows == 0:\n",
    "                break\n",
    "            else:\n",
    "                nrows = nrows - 1\n",
    "        \n",
    "        results = results + \"%s : %s\\n\\n\" % (real_x, gen_x)\n",
    "    print(\"BLEU: %.4f\" % np.mean(bleu))\n",
    "    print(results)\n",
    "\n",
    "generate_output(run.model, bpe, q_july,idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5259400920634215\n"
     ]
    }
   ],
   "source": [
    "q = run.encoder.predict(q_july)\n",
    "d = run.encoder.predict(d_july)\n",
    "\n",
    "cosine = CosineSim(q.shape[-1])\n",
    "pred = cosine.model.predict([q, d])\n",
    "print(auc(qrel_july, pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5594298294380573\n",
      "0.5548266583646512\n",
      "0.5579176766216727\n",
      "0.5607863194645201\n",
      "0.5666347928573017\n",
      "0.552069203228891\n",
      "0.5657897662834397\n",
      "0.5624986101536614\n",
      "0.5639218128043764\n",
      "0.5621650470324001\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    q = run.encoder2.predict(q_july)\n",
    "    d = run.encoder2.predict(d_july)\n",
    "\n",
    "    cosine = CosineSim(q.shape[-1])\n",
    "    pred = cosine.model.predict([q, d])\n",
    "    print(auc(qrel_july, pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v = Sequential()\n",
    "w2v.add(bpe.get_keras_embedding(train_embeddings=True))\n",
    "w2v.add(GlobalMaxPooling1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_gen = w2v.predict(np.argmax(run.model.predict([q_july,q_july]), axis=-1)[0])\n",
    "d_gen = w2v.predict(np.argmax(run.model.predict([d_july, d_july]), axis=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine = CosineSim(q_gen.shape[-1])\n",
    "pred2 = cosine.model.predict([q_gen, d_gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5128421801685606\n"
     ]
    }
   ],
   "source": [
    "print(auc(qrel_july, pred2.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "153c5755047b48ebb7832e01eb6d9ae0": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "27df7fa3080d43978fdec3dd67647769": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "2a368a6564464c7ea4d011063fa57495": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "3826a4d3fd974329b289fb7483e78a8e": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "69ead1824fb345d8ac447d23baee6877": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "8ddcbb3ef69a461c9c7ba24dc7afda7b": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "91512b459c2d476bac3404065c24f45e": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "c548c989b6a34a6bbb0d93d73810cae7": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "c55622033e684412a43270ae60a7d83e": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "c5a9fa3757154d26bff1c2acee0ea685": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "cb370d51edf344c0900bca8b825861a5": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "cea775c9d1bc42f8893e734447ed9e68": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "eac4c239f6734690829f53112534ab56": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "f347de96357f43d7a8d0bf26864df869": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "f7e30b58347c4224a01aff67821eb17a": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "fb80dabeb38b4b129a3967bd8b1d17c4": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
