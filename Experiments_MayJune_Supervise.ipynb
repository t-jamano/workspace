{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Dense, Concatenate,Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import objectives, backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import imdb\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "import pytrec_eval\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE = '/data/t-mipha/data/agi_encoder/v4/universal/CLICKED_QQ_EN_universal_train_1M.txt'\n",
    "df = pd.read_csv(TRAIN_DATA_FILE, usecols=[0,1], names=[\"q\", \"d\"], sep=\"\\t\", header=None, error_bad_lines=False)\n",
    "df = df.dropna()\n",
    "\n",
    "q_train = df.q.tolist()\n",
    "# use only first similar query\n",
    "d_train = [i.split(\"<sep>\")[0] for i in df.d.tolist()]\n",
    "y_train = np.ones(len(df))\n",
    "\n",
    "texts = q_train + d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir = '/data/t-mipha/data/query_similarity_ndcg/MayFlowerIdeal.txt'\n",
    "df_test = pd.read_csv(file_dir, names=[\"market\", \"qid\", \"q\", \"label\", \"d\", \"date\"], sep=\"\\t\", header=0, error_bad_lines=False)\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average query's length: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"Average query's length: {}\".format(np.mean(list(map(len, texts)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 132 ms, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from l3wtransformer import L3wTransformer\n",
    "tokeniser = L3wTransformer()\n",
    "tokeniser.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.max_ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train = tokeniser.texts_to_sequences(q_train)\n",
    "d_train = tokeniser.texts_to_sequences(d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max query's length: 23\n"
     ]
    }
   ],
   "source": [
    "max_len = np.max(list(map(len, q_train + d_train)))\n",
    "print(\"Max query's length: {}\".format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train = pad_sequences(q_train, maxlen=max_len)\n",
    "d_train = pad_sequences(d_train, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_test = pad_sequences(tokeniser.texts_to_sequences(df_test.q.tolist()), maxlen=max_len)\n",
    "d_test = pad_sequences(tokeniser.texts_to_sequences(df_test.d.tolist()), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = np.array([0 if i == \"Bad\" else 1 if i == \"Fare\" else 2 for i in df_test.label.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self, max_len=10, emb_dim=100, nb_words=50000, lstm_dim=256):\n",
    "\n",
    "        q_input = Input(shape=(max_len,))\n",
    "        d_input = Input(shape=(max_len,))\n",
    "        \n",
    "        emb = Embedding(nb_words, emb_dim, mask_zero=True)\n",
    "\n",
    "        lstm = LSTM(lstm_dim)\n",
    "\n",
    "        self.q_embed = lstm(emb(q_input))\n",
    "        self.d_embed = lstm(emb(d_input))\n",
    "\n",
    "        concat = Concatenate()([self.q_embed, self.d_embed])\n",
    "        \n",
    "        self.encoder = Model(q_input, self.q_embed)\n",
    "\n",
    "        pred = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "        self.model = Model(inputs=[q_input, d_input], outputs=pred)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = tokeniser.max_ngrams + 5 + 1     # default l2wtransformer's max vocab = 50K + 5 for unknown word\n",
    "MAX_SEQUENCE_LENGTH = max_len\n",
    "EMBEDDING_DIM = 200 # similar to Bing pre-trained w2v\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 1911s - loss: 0.6932 - acc: 0.5012\n",
      "NDCG: 0.482409\n",
      "MAP: 0.452641\n",
      "NDCG: 0.484091\n",
      "MAP: 0.455093\n",
      "Epoch 1/1\n",
      " - 1921s - loss: 0.6932 - acc: 0.5001\n",
      "NDCG: 0.484053\n",
      "MAP: 0.454987\n",
      "NDCG: 0.483480\n",
      "MAP: 0.454120\n",
      "Epoch 1/1\n",
      " - 1799s - loss: 0.6932 - acc: 0.4997\n",
      "NDCG: 0.484978\n",
      "MAP: 0.456440\n",
      "NDCG: 0.486994\n",
      "MAP: 0.458845\n",
      "Epoch 1/1\n",
      " - 1882s - loss: 0.6932 - acc: 0.5002\n",
      "NDCG: 0.479769\n",
      "MAP: 0.449252\n",
      "NDCG: 0.479720\n",
      "MAP: 0.449187\n",
      "Epoch 1/1\n",
      " - 1930s - loss: 0.6932 - acc: 0.5011\n",
      "NDCG: 0.484907\n",
      "MAP: 0.455871\n",
      "NDCG: 0.485659\n",
      "MAP: 0.456886\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM_Model(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, MAX_NB_WORDS)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    d_neg_train = np.copy(d_train)\n",
    "    np.random.shuffle(d_neg_train)\n",
    "    \n",
    "    q_ = np.concatenate([q_train, q_train])\n",
    "    d_ = np.concatenate([d_train, d_neg_train])\n",
    "    y_ = np.concatenate([np.ones(len(q_train)), np.zeros(len(q_train))])\n",
    "    \n",
    "#     nb = 100000\n",
    "    \n",
    "    q, y = shuffle(q_, y_, random_state=0)\n",
    "    d, y = shuffle(d_, y_, random_state=0)\n",
    "    \n",
    "#     q = q[:nb]\n",
    "#     d = d[:nb]\n",
    "#     y = y[:nb]\n",
    "    \n",
    "    \n",
    "    \n",
    "    lstm.model.fit([q, d], y , batch_size=128, epochs=1, verbose=2)\n",
    "\n",
    "    pred = lstm.model.predict([q_test, d_test])\n",
    "    evaluate(qrel, pred)\n",
    "    pred = get_cosine_sim(lstm.encoder.predict(q_test), lstm.encoder.predict(d_test))\n",
    "    evaluate(qrel, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self, max_len=10, emb_dim=100, nb_words=50000, lstm_dim=256):\n",
    "\n",
    "        q_input = Input(shape=(max_len,))\n",
    "        d_input = Input(shape=(max_len,))\n",
    "        \n",
    "        emb = Embedding(nb_words, emb_dim, mask_zero=True)\n",
    "\n",
    "        lstm = LSTM(lstm_dim)\n",
    "\n",
    "        self.q_embed = lstm(emb(q_input))\n",
    "        self.d_embed = lstm(emb(d_input))\n",
    "\n",
    "        concat = Concatenate()([self.q_embed, self.d_embed])\n",
    "        \n",
    "        self.encoder = Model(q_input, self.q_embed)\n",
    "\n",
    "        pred = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "        self.model = Model(inputs=[q_input, d_input], outputs=pred)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = tokeniser.max_ngrams + 5 + 1     # default l2wtransformer's max vocab = 50K + 5 for unknown word\n",
    "MAX_SEQUENCE_LENGTH = max_len\n",
    "EMBEDDING_DIM = 200 # similar to Bing pre-trained w2v\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 19164, 33897, 50003],\n",
       "       [    0,     0,     0, ..., 19164, 33897, 50003],\n",
       "       [30359, 50003,  1162, ..., 14134, 35818, 50003],\n",
       "       ..., \n",
       "       [    0,     0,     0, ..., 23810, 35067, 50003],\n",
       "       [    0,     0,     0, ..., 23810, 35067, 50003],\n",
       "       [    0,     0,     0, ..., 23810, 35067, 50003]], dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trigram, tokeniser trained on train data\n",
    "NDCG: 0.501180\n",
    "MAP: 0.477413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bad     9804\n",
       "Good    2719\n",
       "Fair    2609\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.478829\n",
      "MAP: 0.448108\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_cosine_sim(x, y):\n",
    "    tmp = []\n",
    "    for i,j in zip(x,y):\n",
    "        tmp.append(cosine_similarity(i.reshape(1, -1),j.reshape(1, -1)))\n",
    "    return np.array(tmp).flatten()\n",
    "\n",
    "def auc(y_test, pred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, pred, pos_label=1)\n",
    "    return metrics.auc(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_2_trec(query, document, label, isQrel):\n",
    "    trec = {}\n",
    "    for i, j, k in zip(query, document, label):\n",
    "        if i not in trec:\n",
    "            trec[i] = {}\n",
    "        if j not in trec[i]:\n",
    "            trec[i][j] = {}\n",
    "        trec[i][j] = int(k) if isQrel else float(k)\n",
    "    return trec\n",
    "\n",
    "def evaluate(qrel, pred):\n",
    "\n",
    "    run = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "        qrel, {'map', 'ndcg'})\n",
    "\n",
    "    results = evaluator.evaluate(run)\n",
    "    print(\"NDCG: %f\" % np.mean([i['ndcg'] for i in results.values()]))\n",
    "    print(\"MAP: %f\" % np.mean([i['map'] for i in results.values()]))\n",
    "    \n",
    "qrel = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.506189\n",
      "MAP: 0.483753\n"
     ]
    }
   ],
   "source": [
    "pred = get_cosine_sim(q_test, d_test)\n",
    "run = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "    qrel, {'map', 'ndcg'})\n",
    "\n",
    "results = evaluator.evaluate(run)\n",
    "print(\"NDCG: %f\" % np.mean([i['ndcg'] for i in results.values()]))\n",
    "print(\"MAP: %f\" % np.mean([i['map'] for i in results.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 12.8 s, total: 40.3 s\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "with open(r\"/data/t-mipha/data/agi_encoder/v4/universal/embedding_dict.pkl\", \"rb\") as input_file:\n",
    "    e = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10923 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = len(e['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "w2v = Sequential()\n",
    "w2v.add(embedding_layer)\n",
    "w2v.add(GlobalAveragePooling1D())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_test = pad_sequences(tokenizer.texts_to_sequences(df_test.q.tolist()), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "d_test = pad_sequences(tokenizer.texts_to_sequences(df_test.d.tolist()), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.522923\n",
      "MAP: 0.507142\n"
     ]
    }
   ],
   "source": [
    "w2v = Sequential()\n",
    "w2v.add(embedding_layer)\n",
    "w2v.add(GlobalAveragePooling1D())\n",
    "\n",
    "pred = get_cosine_sim(w2v.predict(q_test), w2v.predict(d_test))\n",
    "run = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "\n",
    "results = evaluator.evaluate(run)\n",
    "print(\"NDCG: %f\" % np.mean([i['ndcg'] for i in results.values()]))\n",
    "print(\"MAP: %f\" % np.mean([i['map'] for i in results.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827361183641\n",
      "0.745250255879\n"
     ]
    }
   ],
   "source": [
    "w2v = Sequential()\n",
    "w2v.add(embedding_layer)\n",
    "w2v.add(GlobalMaxPooling1D())\n",
    "\n",
    "pred = get_cosine_sim(w2v.predict(q_test), w2v.predict(d_test))\n",
    "run = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "\n",
    "results2 = evaluator.evaluate(run)\n",
    "print(np.mean([i['ndcg'] for i in results2.values()]))\n",
    "print(np.mean([i['map'] for i in results2.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def ttest(res1, res2, metric=\"ndcg\"):\n",
    "    res1 = [i[metric] for i in res1.values()]\n",
    "    res2 = [i[metric] for i in res2.values()]\n",
    "    print(scipy.stats.ttest_rel(res1, res2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
