{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Dense, Concatenate, Embedding, Merge, merge, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import objectives, backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import imdb\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "import pytrec_eval\n",
    "from sklearn.utils import shuffle\n",
    "from Utils import *\n",
    "from Models import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 19898: expected 6 fields, saw 8\\nSkipping line 20620: expected 6 fields, saw 8\\nSkipping line 38039: expected 6 fields, saw 8\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tokeniser\n",
      "Average query's length: 23\n",
      "CPU times: user 3min 7s, sys: 2.01 s, total: 3min 9s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_train, d_train, y_train, q_test, d_test, y_test, qrel, df, df_test, max_len, nb_words = get_data(sup_train_data=\"1M_qq_log\", test_data=\"JuneFlower\", tokenize= \"trigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.2 s, sys: 48.9 s, total: 54.1 s\n",
      "Wall time: 54.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_train_one_hot = to_categorical(q_train, num_classes=nb_words)\n",
    "# d_train_one_hot = to_categorical(d_train, num_classes=nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train_one_hot = q_train_one_hot.reshape(q_train.shape[0], max_len, nb_words)\n",
    "# d_train_one_hot = d_train_one_hot.reshape(d_train.shape[0], max_len, nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1583346160 into shape (996854,23,50005)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-483577cd79a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_train_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/work/data/numpy/1M_qq_log_q_train_one_hot.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/t-jamano/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 421\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/t-jamano/.local/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1583346160 into shape (996854,23,50005)"
     ]
    }
   ],
   "source": [
    "q_train_one_hot = np.load(\"/work/data/numpy/1M_qq_log_q_train_one_hot.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"/work/data/numpy/1M_qq_log_d_train_one_hot\", d_train_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and Tokenise texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://alexadam.ca/ml/2017/05/05/keras-vae.html\n",
    "class EMB_LSTM_VAE():\n",
    "    def __init__(self, vocab_size=50000, max_length=300, latent_rep_size=50):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.sentiment_predictor = None\n",
    "        self.autoencoder = None\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_rep_size = latent_rep_size\n",
    "\n",
    "        x = Input(shape=(max_length,))\n",
    "        self.x_embed = Embedding(vocab_size, 100, input_length=max_length, mask_zero=True)(x)\n",
    "\n",
    "        vae_loss, encoded = self._build_encoder(self.x_embed, latent_rep_size=latent_rep_size, max_length=max_length)\n",
    "        self.encoder = Model(x, encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "\n",
    "        decoded = self._build_decoder(encoded_input, vocab_size, max_length)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.model = Model(x, self._build_decoder(encoded, vocab_size, max_length))\n",
    "\n",
    "        self.model.compile(optimizer='Adam',\n",
    "                                 loss=vae_loss)\n",
    "        \n",
    "    def _build_encoder(self, x, latent_rep_size=200, max_length=300, epsilon_std=0.01):\n",
    "        h = LSTM(500, name='lstm_1')(x)\n",
    "\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "    \n",
    "    def _build_decoder(self, encoded, vocab_size, max_length):\n",
    "        \n",
    "        repeated_context = RepeatVector(max_length)(encoded)\n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "# emb_lstm_vae.model.fit(X_train_, X_train_, verbose=2, batch_size=batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CosineSim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d3f7702c2feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCosineSim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'CosineSim' is not defined"
     ]
    }
   ],
   "source": [
    "# cosine = CosineSim(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "emb_lstm_vae = EMB_LSTM_VAE(nb_words, max_len)  \n",
    "emb_lstm_vae.model.fit(q_train, q_train_one_hot, verbose=2, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=batch_size, iterator=True, sep=\"\\t\", usecols=[0,1,3], names=['label', 'q', 'd'], header=None , error_bad_lines=False)\n",
    "\n",
    "def sent_generator(reader, tokeniser, batch_size, max_len, feature_num):\n",
    "    for df in reader:\n",
    "        q = pad_sequences(l3wt.texts_to_sequences(df.q.tolist()), maxlen=max_len)\n",
    "\n",
    "        q_one_hot = to_categorical(q, feature_num)   \n",
    "        q_one_hot = q_one_hot.reshape(batch_size, max_len, feature_num)\n",
    "        \n",
    "        \n",
    "        yield q, q_one_hot\n",
    "#         print(q_one_hot.shape, q.shape)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " - 1173s - loss: 0.0018\n",
      "Epoch 2/2\n",
      " - 1155s - loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d37db7dd8>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA_FILE = \"/data/chzho/deepqts/train_data/unifiedclick/join_oneyearsample_2B_training_all_top10\"\n",
    "batch_size = 128\n",
    "max_len = 10\n",
    "feature_num = 50005\n",
    "\n",
    "reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=batch_size, iterator=True, sep=\"\\t\", usecols=[0,1,3], names=['label', 'q', 'd'], header=None , error_bad_lines=False)\n",
    "\n",
    "def sent_generator(reader, tokeniser, batch_size, max_len, feature_num):\n",
    "    for df in reader:\n",
    "        q = pad_sequences(tokeniser.texts_to_sequences(df.q.tolist()), maxlen=max_len)\n",
    "\n",
    "        q_one_hot = to_categorical(q, feature_num)   \n",
    "        q_one_hot = q_one_hot.reshape(batch_size, max_len, feature_num)\n",
    "        \n",
    "        \n",
    "        yield q, q_one_hot\n",
    "\n",
    "\n",
    "emb_lstm_vae = EMB_LSTM_VAE(NUM_WORDS, max_len)  \n",
    "emb_lstm_vae.model.fit_generator(sent_generator(reader, tokeniser, batch_size, max_len, feature_num), steps_per_epoch=100, epochs=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 86390s - loss: 7.4557e-04\n",
      "0.533634281394\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "emb_lstm_vae = EMB_LSTM_VAE(NUM_WORDS, max_len)  \n",
    "for i in range(10):\n",
    "    emb_lstm_vae.model.fit_generator(sent_generator(reader, tokeniser, batch_size, max_len, feature_num), steps_per_epoch=10000, epochs=1, verbose=2)\n",
    "    pred = get_cosine_sim(emb_lstm_vae.encoder.predict(q1_qq_test), emb_lstm_vae.encoder.predict(q2_qq_test))\n",
    "    print(auc(df_qq.label.values, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query-Query data\n",
    "q1_qq_test = pad_sequences(l3wt.texts_to_sequences(df_qq.q.tolist()), maxlen=max_len)\n",
    "q2_qq_test = pad_sequences(l3wt.texts_to_sequences(df_qq.d.tolist()), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_cosine_sim(x, y):\n",
    "    tmp = []\n",
    "    for i,j in zip(x,y):\n",
    "        tmp.append(cosine_similarity(i.reshape(1, -1),j.reshape(1, -1)))\n",
    "    return np.array(tmp).flatten()\n",
    "\n",
    "def auc(y_test, pred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, pred, pos_label=1)\n",
    "    return metrics.auc(fpr, tpr)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49560808557\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525072827948\n"
     ]
    }
   ],
   "source": [
    "pred = get_cosine_sim(q1_qq_test, q2_qq_test)\n",
    "print(auc(df_qq.label.values, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10) (?, 10, 50005)\n"
     ]
    }
   ],
   "source": [
    "# https://nicgian.github.io/text-generation-vae/\n",
    "batch_size = 100\n",
    "max_len = 10\n",
    "emb_dim = 100\n",
    "latent_dim = 32\n",
    "intermediate_dim = 96\n",
    "epsilon_std = 1.0\n",
    "num_sampled=500\n",
    "act = ELU()\n",
    "NUM_WORDS = feature_num\n",
    "\n",
    "x = Input(batch_shape=(None, max_len))\n",
    "x_embed = Embedding(NUM_WORDS, emb_dim, input_length=max_len)(x)\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "h = Dropout(0.2)(h)\n",
    "h = Dense(intermediate_dim, activation='linear')(h)\n",
    "h = act(h)\n",
    "h = Dropout(0.2)(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "#     batch_size = z_mean.shape[0]\n",
    "#     latent_dim = z_mean.shape[1]\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "repeated_context = RepeatVector(max_len)\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "decoder_mean = TimeDistributed(Dense(NUM_WORDS, activation='linear'))#softmax is applied in the seq2seqloss by tf\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "\n",
    "# Custom VAE loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
    "                                                     weights=self.target_weights,\n",
    "                                                     average_across_timesteps=False,\n",
    "                                                     average_across_batch=False), axis=-1)\n",
    "        \n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "\n",
    "encoder = Model(x, z)\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "opt = Adam(lr=0.01) #SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "vae.compile(optimizer='adam', loss=[zero_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 354s - loss: 1721.0646\n",
      "Epoch 2/10\n",
      " - 354s - loss: 1531.4780\n",
      "Epoch 3/10\n",
      " - 353s - loss: 1529.2349\n",
      "Epoch 4/10\n",
      " - 349s - loss: 1527.8187\n",
      "Epoch 5/10\n",
      " - 350s - loss: 1526.2863\n",
      "Epoch 6/10\n",
      " - 348s - loss: 1526.0963\n",
      "Epoch 7/10\n",
      " - 348s - loss: 1525.4968\n",
      "Epoch 8/10\n",
      " - 350s - loss: 1525.1179\n",
      "Epoch 9/10\n",
      " - 348s - loss: 1524.6072\n",
      "Epoch 10/10\n",
      " - 350s - loss: 1524.4047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f695cf93be0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(X_train, X_train, verbose=2, batch_size=batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_indices = np.random.choice(np.arange(X_train.shape[0]), 2000, replace=False)\n",
    "# test_indices = np.random.choice(np.arange(X_test.shape[0]), 1000, replace=False)\n",
    "\n",
    "# X_train_ = X_train[train_indices]\n",
    "# y_train_ = y_train[train_indices]\n",
    "\n",
    "# X_test_ = X_test[test_indices]\n",
    "# y_test_ = y_test[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.zeros((X_train.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_train.shape[0]), axis=0).reshape(X_train.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_train.shape[0], axis=0), X_train] = 1\n",
    "\n",
    "X_train_one_hot = temp\n",
    "\n",
    "temp = np.zeros((X_test.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_test.shape[0]), axis=0).reshape(X_test.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_test.shape[0], axis=0), X_test] = 1\n",
    "\n",
    "x_test_one_hot = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_ca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
