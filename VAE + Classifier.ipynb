{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Models import *\n",
    "from Utils import *\n",
    "from FastModels import *\n",
    "import warnings\n",
    "from livelossplot import PlotLossesKeras\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import sentencepiece as spm\n",
    "\n",
    "max_len = 20\n",
    "enablePadding = True\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/work/data/bpe/en.wiki.bpe.op50000.model')\n",
    "bpe = KeyedVectors.load_word2vec_format(\"/work/data/bpe/en.wiki.bpe.op50000.d200.w2v.bin\", binary=True)\n",
    "bpe.index2word = [''] + bpe.index2word + ['<sos>'] + ['<eos>']  # add empty string \n",
    "nb_words = len(bpe.index2word)\n",
    "# word2index\n",
    "bpe_dict = {bpe.index2word[i]: i for i in range(len(bpe.index2word))}\n",
    "# construct embedding_matrix\n",
    "embedding_matrix = np.concatenate([np.zeros((1, bpe.vector_size)), bpe.vectors, np.zeros((2, bpe.vector_size))]) # add zero vector for empty string (i.e. used for padding)\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "                    embedding_matrix.shape[-1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 19898: expected 6 fields, saw 8\\nSkipping line 20620: expected 6 fields, saw 8\\nSkipping line 38039: expected 6 fields, saw 8\\n'\n"
     ]
    }
   ],
   "source": [
    "enablePadding = True\n",
    "df_may, qrel_may = get_test_data(\"MayFlower\", \"/work/\")\n",
    "df_june, qrel_june = get_test_data(\"JuneFlower\", \"/work/\")\n",
    "df_july, qrel_july = get_test_data(\"JulyFlower\", \"/work/\")\n",
    "\n",
    "q_may = parse_texts_bpe(df_may.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_may = parse_texts_bpe(df_may.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "q_june = parse_texts_bpe(df_june.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_june = parse_texts_bpe(df_june.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "q_july = parse_texts_bpe(df_july.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_july = parse_texts_bpe(df_july.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "q_july_ = parse_texts_bpe(df_july.q.tolist(), sp, bpe_dict, max_len, enablePadding, \"post\")\n",
    "d_july_ = parse_texts_bpe(df_july.d.tolist(), sp, bpe_dict, max_len, enablePadding, \"post\")\n",
    "\n",
    "test_set = [[q_may, d_may, qrel_may, df_may, \"MayFlower\"], [q_june, d_june, qrel_june, df_june, \"JuneFlower\"], [q_july, d_july, qrel_july, df_july, \"JulyFlower\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_july(run):\n",
    "    q_ = run.predict(q_july)\n",
    "    d_ = run.predict(d_july)\n",
    "    cosine = CosineSim(q_.shape[-1])\n",
    "\n",
    "    pred = cosine.model.predict([q_, d_])\n",
    "    return auc(qrel_july, pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/work/data/train_data/30M_QD_lower2.txt\", nrows=1000000, names=[\"label\", \"q\", \"d\"], sep=\"\\t\", header=None, error_bad_lines=False)\n",
    "\n",
    "q_df = parse_texts_bpe(df.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_df = parse_texts_bpe(df.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "q_df_ = parse_texts_bpe(df.q.tolist(), sp, bpe_dict, max_len, enablePadding, \"post\")\n",
    "d_df_ = parse_texts_bpe(df.d.tolist(), sp, bpe_dict, max_len, enablePadding, \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "hidden_dim, latent_dim = 200, 200\n",
    "optimizer = Adam()\n",
    "\n",
    "dssm = DSSM(hidden_dim, latent_dim, 1, nb_words, max_len, embedding_matrix, optimizer=optimizer, enableLSTM=True, enableSeparate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step = 100 * 256\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for i in range(0, len(q_df), step):\n",
    "        \n",
    "        q = q_df[i: i + step]\n",
    "        d = d_df[i: i + step]\n",
    "    \n",
    "        idx = np.arange(len(q))\n",
    "        shuffle(idx)\n",
    "        \n",
    "        x_train = [q, d, d[idx]]\n",
    "        y_train = np.zeros((len(q), 2))\n",
    "        y_train[:, 0] = 1\n",
    "        \n",
    "        hist = dssm.model.fit(x_train, y_train, verbose=0, batch_size=256)\n",
    "        print(\"Epoch %d, Loss %.4f, AUC %.4f\" % (epoch, hist.history['loss'][-1], eval_july(dssm.encoder)))\n",
    "        \n",
    "        if i > 100000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train DSSM_LSTM on 1M Q-D paired data\n",
    "- Train SeqVAE on 1M on 1M Q data\n",
    "- Train SeqVAE on 1M on 1M D data\n",
    "- intialise DSSM_LSTM using SeqVAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SeqVAE(object):\n",
    "    \n",
    "    def __init__(self, nb_words, max_len, embedding_matrix, dim, optimizer=Adam(), keep_rate_word_dropout=0.5, kl_weight=0, enableKL=False):\n",
    "        self.dim = dim\n",
    "        self.nb_words = nb_words\n",
    "        self.max_len = max_len\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.optimizer = optimizer\n",
    "        self.kl_weight = kl_weight\n",
    "        self.keep_rate_word_dropout = keep_rate_word_dropout\n",
    "        self.enableKL = enableKL\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "                \n",
    "        e_input = Input(shape=(self.max_len,))\n",
    "        self.kl_input = Input(shape=(1,))\n",
    "\n",
    "        embedding_layer = Embedding(self.nb_words,\n",
    "                            self.embedding_matrix.shape[-1],\n",
    "                            weights=[self.embedding_matrix],\n",
    "                            input_length=self.max_len,\n",
    "                            mask_zero=True,\n",
    "                            trainable=True)\n",
    "\n",
    "        lstm = GRU(self.dim[0], return_state=True, return_sequences=True)\n",
    "        \n",
    "        _, h = lstm(embedding_layer(e_input))\n",
    "\n",
    "        \n",
    "        mean = Dense(self.dim[1])\n",
    "        var = Dense(self.dim[1])\n",
    "        \n",
    "        self.h_mean = mean(h)\n",
    "        self.h_log_var = var(h)\n",
    "        \n",
    "        z = Lambda(self.sampling)([self.h_mean, self.h_log_var])\n",
    "\n",
    "        self.encoder = Model(e_input, self.h_mean)\n",
    "\n",
    "        d_input = Input(shape=(self.max_len,))\n",
    "        d_latent2hidden = Dense(self.dim[0], activation='relu')\n",
    "        # d_lstm = GRU(self.dim[0], return_sequences=True)\n",
    "\n",
    "        softmax_layer = Dense(self.nb_words, activation=\"softmax\")\n",
    "        d_output2vocab = TimeDistributed(softmax_layer, name=\"rec\")\n",
    "        d_output2vocab_kl = TimeDistributed(softmax_layer, name=\"kl\")\n",
    "        \n",
    "        # dec_embedding_layer = Embedding(self.nb_words,\n",
    "        #                             self.embedding_matrix.shape[-1],\n",
    "        #                             weights=[self.embedding_matrix],\n",
    "        #                             input_length=self.max_len,\n",
    "        #                             mask_zero=True,\n",
    "        #                             trainable=True)\n",
    "\n",
    "        h_z = d_latent2hidden(z)\n",
    "        \n",
    "        d_embed_input = embedding_layer(d_input)\n",
    "        outputs, _ = lstm(d_embed_input, initial_state=[h_z])\n",
    "\n",
    "        pred = d_output2vocab(outputs)\n",
    "        pred_kl = d_output2vocab_kl(outputs)\n",
    "        \n",
    "#       VAE model\n",
    "        self.model = Model(inputs=[e_input, d_input], outputs=[pred])\n",
    "        self.model.compile(optimizer=self.optimizer, loss=[\"sparse_categorical_crossentropy\"])\n",
    "\n",
    "    \n",
    "    def name(self):\n",
    "        \n",
    "        return \"seqvae_kl\" if self.enableKL else \"seqvae_%d\" % self.kl_weight\n",
    "    \n",
    "    def kl_loss(self, x, x_):\n",
    "            kl_loss = - 0.5 * K.sum(1 + self.h_log_var - K.square(self.h_mean) - K.exp(self.h_log_var), axis=-1)\n",
    "            return (self.kl_weight * kl_loss) \n",
    "\n",
    "    def word_dropout(self, x, unk_token):\n",
    "\n",
    "        x_ = np.copy(x)\n",
    "        rows, cols = np.nonzero(x_)\n",
    "        for r, c in zip(rows, cols):\n",
    "            if random.random() <= self.keep_rate_word_dropout:\n",
    "                continue\n",
    "            x_[r][c] = unk_token\n",
    "\n",
    "        return x_\n",
    "    \n",
    "\n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.dim[1]), mean=0.,\\\n",
    "                                  stddev=1)\n",
    "    \n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run = SeqVAE(nb_words, max_len, embedding_matrix, [200,200], optimizer=Adam(), kl_weight=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17920 samples, validate on 7680 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 1.6718 - val_loss: 2.6919\n",
      "Epoch 0, Loss 1.6718\n",
      "Train on 17920 samples, validate on 7680 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 1.6074 - val_loss: 2.6538\n",
      "Epoch 0, Loss 1.6074\n",
      "Train on 17920 samples, validate on 7680 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 1.5992 - val_loss: 2.6229\n",
      "Epoch 0, Loss 1.5992\n",
      "Train on 17920 samples, validate on 7680 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 1.5779 - val_loss: 2.5381\n",
      "Epoch 0, Loss 1.5779\n",
      "Train on 17920 samples, validate on 7680 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 1.5336 - val_loss: 2.6094\n",
      "Epoch 0, Loss 1.5336\n",
      "Train on 17920 samples, validate on 7680 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 1.5128 - val_loss: 2.5215\n",
      "Epoch 0, Loss 1.5128\n",
      "Train on 17920 samples, validate on 7680 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 1.5019 - val_loss: 2.5332\n",
      "Epoch 0, Loss 1.5019\n",
      "Train on 17920 samples, validate on 7680 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1e46b861e6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d, Loss %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/t-jamano/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/t-jamano/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/t-jamano/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 100 * 256\n",
    "for epoch in range(5):\n",
    "    for i in range(0, len(q_df), step):\n",
    "\n",
    "        enc_input = q_df[i: i + step]\n",
    "        dec_output = q_df_[i: i + step]\n",
    "        dec_input = pad_sequences(pad_sequences(dec_output, maxlen=max_len+1, value=bpe_dict['<sos>']), maxlen=max_len, truncating=\"post\")\n",
    "\n",
    "        x_train = [enc_input, dec_input]\n",
    "        _ = np.expand_dims(dec_output, axis=-1)\n",
    "        y_train = [_]\n",
    "\n",
    "        hist = run.model.fit(x_train, y_train, verbose=2, batch_size=256, validation_split=0.3)\n",
    "        print(\"Epoch %d, Loss %.4f\" % (epoch, hist.history['loss'][-1]))\n",
    "        \n",
    "        if i > 200000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_weights = run.model.layers[2].get_weights()\n",
    "rnn_weights = run.model.layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"/work/workspace/vae.emb\", embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"/work/workspace/vae.gpu\", rnn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_weights = np.load(\"/work/workspace/vae.emb.npy\")\n",
    "rnn_weights = np.load(\"/work/workspace/vae.gpu.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DSSM():\n",
    "    \n",
    "    def __init__(self, hidden_dim=300, latent_dim=128, num_negatives=1, nb_words=50005, max_len=10, embedding_matrix=None, optimizer=None, enableLSTM=False, enableSeparate=False, PoolMode=\"max\"):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_negatives = num_negatives\n",
    "        self.nb_words = nb_words\n",
    "        self.max_len = max_len\n",
    "        self.enableSeparate = enableSeparate\n",
    "        self.enableLSTM = enableLSTM\n",
    "        self.PoolMode = PoolMode\n",
    "\n",
    "        # Input tensors holding the query, positive (clicked) document, and negative (unclicked) documents.\n",
    "        # The first dimension is None because the queries and documents can vary in length.\n",
    "        query = Input(shape = (self.max_len,))\n",
    "        pos_doc = Input(shape = (self.max_len,))\n",
    "        neg_docs = [Input(shape = (self.max_len,)) for j in range(self.num_negatives)]\n",
    "\n",
    "        embed_layer = Embedding(nb_words,\n",
    "                    embedding_matrix.shape[-1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    mask_zero=True if self.enableLSTM else False,\n",
    "                    trainable=True)\n",
    "        \n",
    "        bilstm = GRU(hidden_dim, name='lstm_1', return_sequences=False)\n",
    "\n",
    "        if enableSeparate:\n",
    "            d_embed_layer = Embedding(nb_words,\n",
    "                    embedding_matrix.shape[-1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    mask_zero=True if self.enableLSTM else False,\n",
    "                    trainable=True)\n",
    "            d_bilstm = GRU(hidden_dim, name=\"lstm_2\")\n",
    "\n",
    "        Pooling = GlobalMaxPooling1D() if self.PoolMode == \"max\" else GlobalAveragePooling1D()\n",
    "\n",
    "        if self.enableLSTM:\n",
    "            query_sem = bilstm(embed_layer(query))\n",
    "            pos_doc_sem = bilstm(embed_layer(pos_doc)) if not enableSeparate else d_bilstm(d_embed_layer(pos_doc))\n",
    "            neg_doc_sems = [bilstm(embed_layer(neg_doc)) for neg_doc in neg_docs] if not enableSeparate else [d_bilstm(d_embed_layer(neg_doc)) for neg_doc in neg_docs]\n",
    "        else:\n",
    "            query_sem = Pooling(embed_layer(query))\n",
    "            pos_doc_sem = Pooling(embed_layer(pos_doc)) if not enableSeparate else Pooling(d_embed_layer(pos_doc))\n",
    "            neg_doc_sems = [Pooling(embed_layer(neg_doc)) for neg_doc in neg_docs] if not enableSeparate else [Pooling(d_embed_layer(neg_doc)) for neg_doc in neg_docs]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # This layer calculates the cosine similarity between the semantic representations of\n",
    "        # a query and a document.\n",
    "        R_Q_D_p = dot([query_sem, pos_doc_sem], axes = 1, normalize = True) # See equation (4).\n",
    "        R_Q_D_ns = [dot([query_sem, neg_doc_sem], axes = 1, normalize = True) for neg_doc_sem in neg_doc_sems] # See equation (4).\n",
    "\n",
    "        concat_Rs = concatenate([R_Q_D_p] + R_Q_D_ns)\n",
    "        concat_Rs = Reshape((self.num_negatives + 1, 1))(concat_Rs)\n",
    "\n",
    "        # In this step, we multiply each R(Q, D) value by gamma. In the paper, gamma is\n",
    "        # described as a smoothing factor for the softmax function, and it's set empirically\n",
    "        # on a held-out data set. We're going to learn gamma's value by pretending it's\n",
    "        # a single 1 x 1 kernel.\n",
    "        weight = np.array([1]).reshape(1, 1, 1)\n",
    "        with_gamma = Convolution1D(1, 1, padding = \"same\", input_shape = (self.num_negatives + 1, 1), activation = \"linear\", use_bias = False, weights = [weight])(concat_Rs) # See equation (5).\n",
    "        with_gamma = Reshape((self.num_negatives + 1, ))(with_gamma)\n",
    "\n",
    "        # Finally, we use the softmax function to calculate P(D+|Q).\n",
    "        prob = Activation(\"softmax\")(with_gamma) # See equation (5).\n",
    "\n",
    "        # We now have everything we need to define our model.\n",
    "        self.model = Model(inputs = [query, pos_doc] + neg_docs, outputs = prob)\n",
    "        self.model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\")\n",
    "\n",
    "        self.encoder = Model(inputs=query, outputs=query_sem)\n",
    "\n",
    "    def name(self):\n",
    "        if self.enableLSTM:\n",
    "            return \"dssm_lstm2\" if self.enableSeparate else \"dssm_lstm\"\n",
    "        else:\n",
    "            return \"dssm2_%s\" % self.PoolMode if self.enableSeparate else \"dssm_%s\" % self.PoolMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dssm2 = DSSM(200, 20, 1, nb_words, max_len, embedding_matrix, optimizer=Adam(), enableLSTM=True, enableSeparate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dssm2.model.layers[3].set_weights(embedding_weights)\n",
    "dssm2.model.layers[4].set_weights(embedding_weights)\n",
    "\n",
    "dssm2.model.layers[5].set_weights(rnn_weights)\n",
    "dssm2.model.layers[6].set_weights(rnn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.4660, AUC 0.5194\n",
      "Epoch 0, Loss 0.3933, AUC 0.5115\n",
      "Epoch 0, Loss 0.3627, AUC 0.5084\n",
      "Epoch 0, Loss 0.3421, AUC 0.5242\n",
      "Epoch 0, Loss 0.3219, AUC 0.5251\n",
      "Epoch 1, Loss 0.3008, AUC 0.5256\n"
     ]
    }
   ],
   "source": [
    "step = 100 * 256\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for i in range(0, len(q_df), step):\n",
    "        \n",
    "        q = q_df[i: i + step]\n",
    "        d = d_df[i: i + step]\n",
    "    \n",
    "        idx = np.arange(len(q))\n",
    "        shuffle(idx)\n",
    "        \n",
    "        x_train = [q, d, d[idx]]\n",
    "        y_train = np.zeros((len(q), 2))\n",
    "        y_train[:, 0] = 1\n",
    "        \n",
    "        hist = dssm2.model.fit(x_train, y_train, verbose=0, batch_size=256)\n",
    "        print(\"Epoch %d, Loss %.4f, AUC %.4f\" % (epoch, hist.history['loss'][-1], eval_july(dssm2.encoder)))\n",
    "        \n",
    "        if i > 100000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
