{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "from keras.layers import Bidirectional, Dense, Embedding, Concatenate, Flatten, Reshape, Input, Lambda, LSTM, merge, GlobalAveragePooling1D, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.merge import concatenate, dot\n",
    "# from keras_tqdm import TQDMNotebookCallback\n",
    "# from keras_tqdm import TQDMCallback\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CosineSim():\n",
    "    def __init__(self, feature_num):\n",
    "        q_input = Input(shape=(feature_num,))\n",
    "        d_input = Input(shape=(feature_num,))\n",
    "\n",
    "        pred = merge([q_input, d_input], mode=\"cos\")\n",
    "        self.model = Model([q_input, d_input], pred)\n",
    "\n",
    "\n",
    "class LSTM_Model():\n",
    "    def __init__(self, max_len=10, emb_dim=100, nb_words=50000):\n",
    "\n",
    "        q_input = Input(shape=(max_len,))\n",
    "        d_input = Input(shape=(max_len,))\n",
    "        \n",
    "        emb = Embedding(nb_words, emb_dim, mask_zero=True)\n",
    "\n",
    "        lstm = LSTM(256)\n",
    "\n",
    "        self.q_embed = lstm(emb(q_input))\n",
    "        self.d_embed = lstm(emb(d_input))\n",
    "\n",
    "        concat = Concatenate()([self.q_embed, self.d_embed])\n",
    "\n",
    "        pred = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "        self.model = Model(inputs=[q_input, d_input], outputs=pred)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self, input_dim):\n",
    "\n",
    "        que_input = Input(shape=(input_dim,))\n",
    "        doc_input = Input(shape=(input_dim,))\n",
    "        \n",
    "        \n",
    "\n",
    "        concat = merge([que_input, doc_input], mode=\"concat\")\n",
    "\n",
    "\n",
    "        pred = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "        self.model = Model(input=[que_input, doc_input], output=pred)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "class W2V_MLP():\n",
    "    def __init__(self, max_len, input_dim):\n",
    "\n",
    "        que_input = Input(shape=(max_len, input_dim,))\n",
    "        doc_input = Input(shape=(max_len, input_dim,))\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(que_input)\n",
    "        y = GlobalAveragePooling1D()(doc_input)\n",
    "\n",
    "        concat = merge([x, y], mode=\"concat\")\n",
    "\n",
    "\n",
    "        pred = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "        self.model = Model(input=[que_input, doc_input], output=pred)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "class AVGPolling():\n",
    "    def __init__(self, max_len, emb_dim):\n",
    "        x_input = Input(shape=(max_len, emb_dim,))\n",
    "        avg = GlobalAveragePooling1D()(x_input)\n",
    "        self.model = Model(input=x_input, output=avg)\n",
    "\n",
    "\n",
    "\n",
    "# http://alexadam.ca/ml/2017/05/05/keras-vae.html\n",
    "class EMB_LSTM_VAE():\n",
    "    def __init__(self, vocab_size=50000, max_length=300, latent_rep_size=50):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.sentiment_predictor = None\n",
    "        self.autoencoder = None\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_rep_size = latent_rep_size\n",
    "\n",
    "        x = Input(shape=(max_length,))\n",
    "        self.x_embed = Embedding(vocab_size, 100, input_length=max_length, mask_zero=True)(x)\n",
    "\n",
    "        vae_loss, encoded = self._build_encoder(self.x_embed, latent_rep_size=latent_rep_size, max_length=max_length)\n",
    "        self.encoder = Model(x, encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "\n",
    "        decoded = self._build_decoder(encoded_input, vocab_size, max_length)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.model = Model(x, self._build_decoder(encoded, vocab_size, max_length))\n",
    "\n",
    "        self.model.compile(optimizer='Adam',\n",
    "                                 loss=vae_loss)\n",
    "        \n",
    "    def _build_encoder(self, x, latent_rep_size=200, max_length=300, epsilon_std=0.01):\n",
    "        h = LSTM(200, name='lstm_1')(x)\n",
    "\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            latent_dim = K.sahep(z_mean_)[1]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "    \n",
    "    def _build_decoder(self, encoded, vocab_size, max_length):\n",
    "        \n",
    "        repeated_context = RepeatVector(max_length)(encoded)\n",
    "        h = LSTM(200, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "\n",
    "class VAE_BPE():\n",
    "\n",
    "    def __init__(self, hidden_dim=300, latent_dim=128, nb_words=50005, max_len=10, emb=None, emb_dim=200, activation=\"relu\"):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.nb_words = nb_words\n",
    "        self.activation = activation\n",
    "        self.emb_dim = emb_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "        x = Input(shape=(self.max_len,))\n",
    "        \n",
    "        embed_layer = Embedding(self.nb_words, self.emb_dim, input_length=self.max_len) if emb == None else emb\n",
    "\n",
    "\n",
    "        emb_x = embed_layer(x)\n",
    "\n",
    "        vae_loss, encoded = self.build_encoder(emb_x)\n",
    "        self.encoder = Model(x, encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(self.latent_dim,))\n",
    "\n",
    "        decoded = self.build_decoder(encoded_input)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.model = Model(x, self.build_decoder(encoded))\n",
    "\n",
    "        self.model.compile(optimizer='Adam',\n",
    "                                 loss=vae_loss)\n",
    "        \n",
    "    def build_encoder(self, z):\n",
    "        \n",
    "        z = LSTM(self.hidden_dim, name='lstm_1')(z)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            latent_dim = K.shape(z_mean_)[1]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean', activation='linear')(z)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var', activation='linear')(z)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = self.max_len * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(self.latent_dim,), name='lambda')([z_mean, z_log_var]))\n",
    "    \n",
    "    def build_decoder(self, encoded):\n",
    "        repeated_context = RepeatVector(self.max_len)(encoded)\n",
    "        h = LSTM(self.hidden_dim, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        decoded = TimeDistributed(Dense(self.nb_words, activation='softmax'), name='decoded_mean')(h)\n",
    "\n",
    "        return decoded\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"vae_bpe_h%d_l%d_w%d_%s_ml%d\" % (self.hidden_dim, self.latent_dim, self.nb_words, self.activation, self.max_len)\n",
    "\n",
    "\n",
    "    def batch_generator(self, reader, train_data, sp, bpe, batch_size):\n",
    "        while True:\n",
    "            for df in reader:\n",
    "                \n",
    "                x = []\n",
    "                for text in df.q.tolist():\n",
    "                    x.append([bpe.index2word.index(t) if t in bpe.index2word else bpe.index2word.index('<unk>') for t in sp.EncodeAsPieces(text)])\n",
    "                \n",
    "                x = pad_sequences(x, maxlen=self.max_len)\n",
    "                x_one_hot = to_categorical(x, self.nb_words)\n",
    "                x_one_hot = x_one_hot.reshape(batch_size, self.max_len, self.nb_words)\n",
    "                \n",
    "                \n",
    "                yield x, x_one_hot\n",
    "                \n",
    "\n",
    "\n",
    "class VAE_DSSM():\n",
    "\n",
    "    def __init__(self, hidden_dim=300, latent_dim=128, nb_words=50005, activation=\"relu\"):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.nb_words = nb_words\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "        x = Input(shape=(self.nb_words,))\n",
    "        enc_dense = Dense(self.hidden_dim, activation=self.activation)\n",
    "\n",
    "        emb_x = enc_dense(x)\n",
    "\n",
    "        vae_loss, encoded = self.build_encoder(emb_x)\n",
    "        self.encoder = Model(x, encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(self.latent_dim,))\n",
    "\n",
    "        decoded = self.build_decoder(encoded_input)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.model = Model(x, self.build_decoder(encoded))\n",
    "\n",
    "        self.model.compile(optimizer='Adam',\n",
    "                                 loss=vae_loss)\n",
    "        \n",
    "    def build_encoder(self, z):\n",
    "\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            latent_dim = K.shape(z_mean_)[1]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean', activation='linear')(z)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var', activation='linear')(z)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(self.latent_dim,), name='lambda')([z_mean, z_log_var]))\n",
    "    \n",
    "    def build_decoder(self, encoded):\n",
    "\n",
    "        h = Dense(self.hidden_dim, activation=self.activation)(encoded)\n",
    "#       output is 2D one-hot vector, sigmoid is appropirate\n",
    "        decoded = Dense(self.nb_words, activation='sigmoid', name='decoded_mean')(h)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"vae_dssm_h%d_l%d_w%d_%s\" % (self.hidden_dim, self.latent_dim, self.nb_words, self.activation)\n",
    "\n",
    "\n",
    "    def batch_generator(self, reader, train_data, tokeniser, batch_size, max_len, nb_words):\n",
    "        while True:\n",
    "            for df in reader:\n",
    "                q = df.q.tolist()\n",
    "#                 if train_data == \"1M_EN_QQ_log\":\n",
    "#                     d = [i.split(\"<sep>\")[0] for i in df.d.tolist()]\n",
    "#                 else:\n",
    "#                     d = df.d.tolist()\n",
    "                \n",
    "                q = pad_sequences(tokeniser.texts_to_sequences(q), maxlen=max_len)\n",
    "                \n",
    "                q_one_hot = np.zeros((batch_size, nb_words))\n",
    "                for i in range(len(q)):\n",
    "                    q_one_hot[i][q[i]] = 1\n",
    "                    \n",
    "                \n",
    "                yield q_one_hot, q_one_hot\n",
    "\n",
    "\n",
    "\n",
    "class CLSM():\n",
    "\n",
    "    # K - hidden layer dimension\n",
    "    # L - latent dimension\n",
    "    # J - number of negative samples\n",
    "    # self.nb_words - feature number / nb_words\n",
    "    \n",
    "    def __init__(self, hidden_dim=300, latent_dim=128, FILTER_LENGTH=1, num_negatives=1,  nb_words=50005, activation=\"relu\"):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.nb_words = nb_words\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "        self.num_negatives = num_negatives\n",
    "\n",
    "        # Input tensors holding the query, positive (clicked) document, and negative (unclicked) documents.\n",
    "        # The first dimension is None because the queries and documents can vary in length.\n",
    "        query = Input(shape = (None, self.nb_words))\n",
    "        pos_doc = Input(shape = (None, self.nb_words))\n",
    "        neg_docs = [Input(shape = (None, self.nb_words)) for j in range(self.num_negatives)]\n",
    "\n",
    "        query_conv = Convolution1D(self.hidden_dim, FILTER_LENGTH, padding = \"same\", input_shape = (None, self.nb_words), activation = \"tanh\")(query) # See equation (2).\n",
    "        query_max = Lambda(lambda x: K.max(x, axis = 1), output_shape = (self.hidden_dim, ))(query_conv) # See section 3.4.\n",
    "\n",
    "        query_sem = Dense(self.latent_dim, activation = \"tanh\", input_dim = self.hidden_dim)(query_max) # See section 3.5.\n",
    "\n",
    "        # The document equivalent of the above query model.\n",
    "        doc_conv = Convolution1D(self.hidden_dim, FILTER_LENGTH, padding = \"same\", input_shape = (None, self.nb_words), activation = \"tanh\")\n",
    "        doc_max = Lambda(lambda x: K.max(x, axis = 1), output_shape = (self.hidden_dim, ))\n",
    "        doc_sem = Dense(self.latent_dim, activation = \"tanh\", input_dim = self.hidden_dim)\n",
    "\n",
    "        pos_doc_conv = doc_conv(pos_doc)\n",
    "        neg_doc_convs = [doc_conv(neg_doc) for neg_doc in neg_docs]\n",
    "\n",
    "        pos_doc_max = doc_max(pos_doc_conv)\n",
    "        neg_doc_maxes = [doc_max(neg_doc_conv) for neg_doc_conv in neg_doc_convs]\n",
    "\n",
    "        pos_doc_sem = doc_sem(pos_doc_max)\n",
    "        neg_doc_sems = [doc_sem(neg_doc_max) for neg_doc_max in neg_doc_maxes]\n",
    "\n",
    "        # This layer calculates the cosine similarity between the semantic representations of\n",
    "        # a query and a document.\n",
    "        R_Q_D_p = dot([query_sem, pos_doc_sem], axes = 1, normalize = True) # See equation (4).\n",
    "        R_Q_D_ns = [dot([query_sem, neg_doc_sem], axes = 1, normalize = True) for neg_doc_sem in neg_doc_sems] # See equation (4).\n",
    "\n",
    "        concat_Rs = concatenate([R_Q_D_p] + R_Q_D_ns)\n",
    "        concat_Rs = Reshape((self.num_negatives + 1, 1))(concat_Rs)\n",
    "\n",
    "        # In this step, we multiply each R(Q, D) value by gamma. In the paper, gamma is\n",
    "        # described as a smoothing factor for the softmax function, and it's set empirically\n",
    "        # on a held-out data set. We're going to learn gamma's value by pretending it's\n",
    "        # a single 1 x 1 kernel.\n",
    "        weight = np.array([1]).reshape(1, 1, 1)\n",
    "        with_gamma = Convolution1D(1, 1, padding = \"same\", input_shape = (self.num_negatives + 1, 1), activation = \"linear\", use_bias = False, weights = [weight])(concat_Rs) # See equation (5).\n",
    "        with_gamma = Reshape((self.num_negatives + 1, ))(with_gamma)\n",
    "\n",
    "        # Finally, we use the softmax function to calculate P(D+|Q).\n",
    "        prob = Activation(\"softmax\")(with_gamma) # See equation (5).\n",
    "\n",
    "        # We now have everything we need to define our model.\n",
    "        self.model = Model(inputs = [query, pos_doc] + neg_docs, outputs = prob)\n",
    "        self.model.compile(optimizer = \"adadelta\", loss = \"categorical_crossentropy\")\n",
    "\n",
    "        self.encoder = Model(inputs=query, outputs=query_sem)\n",
    "\n",
    "    def batch_generator(self, reader, train_data, tokeniser, batch_size, max_len, nb_words):\n",
    "        while True:\n",
    "            for df in reader:\n",
    "                q = df.q.tolist()\n",
    "                if train_data == \"1M_EN_QQ_log\":\n",
    "                    d = [i.split(\"<sep>\")[0] for i in df.d.tolist()]\n",
    "                else:\n",
    "                    d = df.d.tolist()\n",
    "                \n",
    "                q = pad_sequences(tokeniser.texts_to_sequences(q), maxlen=max_len)\n",
    "                d = pad_sequences(tokeniser.texts_to_sequences(d), maxlen=max_len)\n",
    "                \n",
    "                \n",
    "                q_one_hot = to_categorical(q, nb_words)   \n",
    "                q_one_hot = q_one_hot.reshape(batch_size, max_len, nb_words)\n",
    "                \n",
    "                d_one_hot = to_categorical(d, nb_words)   \n",
    "                d_one_hot = q_one_hot.reshape(batch_size, max_len, nb_words)\n",
    "                    \n",
    "                    \n",
    "                # negative sampling from positive pool\n",
    "                neg_d_one_hot = [[] for j in range(self.num_negatives)]\n",
    "                for i in range(batch_size):\n",
    "                    possibilities = list(range(batch_size))\n",
    "                    possibilities.remove(i)\n",
    "                    negatives = np.random.choice(possibilities, self.num_negatives, replace = False)\n",
    "                    for j in range(self.num_negatives):\n",
    "                        negative = negatives[j]\n",
    "                        neg_d_one_hot[j].append(d_one_hot[negative].tolist())\n",
    "                \n",
    "                y = np.zeros((batch_size, self.num_negatives + 1))\n",
    "                y[:, 0] = 1\n",
    "                \n",
    "                for j in range(self.num_negatives):\n",
    "                    neg_d_one_hot[j] = np.array(neg_d_one_hot[j])\n",
    "                \n",
    "                \n",
    "                yield [q_one_hot, d_one_hot] + [neg_d_one_hot[j] for j in range(self.num_negatives)], y\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "class DSSM():\n",
    "    \n",
    "    def __init__(self, hidden_dim=300, latent_dim=128, num_negatives=1, nb_words=50005):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_negatives = num_negatives\n",
    "        self.nb_words = nb_words\n",
    "        # Input tensors holding the query, positive (clicked) document, and negative (unclicked) documents.\n",
    "        # The first dimension is None because the queries and documents can vary in length.\n",
    "        query = Input(shape = (self.nb_words,))\n",
    "        pos_doc = Input(shape = (self.nb_words,))\n",
    "        neg_docs = [Input(shape = (self.nb_words,)) for j in range(self.num_negatives)]\n",
    "\n",
    "        dense = Dense(self.latent_dim, activation = \"tanh\")\n",
    "        query_sem = dense(query)\n",
    "        # query_sem = Dense(L, activation = \"tanh\")(query) # See section 3.5.\n",
    "        # doc_sem = Dense(L, activation = \"tanh\")\n",
    "        # shared dense\n",
    "        doc_sem = dense\n",
    "\n",
    "        pos_doc_sem = doc_sem(pos_doc)\n",
    "        neg_doc_sems = [doc_sem(neg_doc) for neg_doc in neg_docs]\n",
    "\n",
    "        # This layer calculates the cosine similarity between the semantic representations of\n",
    "        # a query and a document.\n",
    "        R_Q_D_p = dot([query_sem, pos_doc_sem], axes = 1, normalize = True) # See equation (4).\n",
    "        R_Q_D_ns = [dot([query_sem, neg_doc_sem], axes = 1, normalize = True) for neg_doc_sem in neg_doc_sems] # See equation (4).\n",
    "\n",
    "        concat_Rs = concatenate([R_Q_D_p] + R_Q_D_ns)\n",
    "        concat_Rs = Reshape((self.num_negatives + 1, 1))(concat_Rs)\n",
    "\n",
    "        # In this step, we multiply each R(Q, D) value by gamma. In the paper, gamma is\n",
    "        # described as a smoothing factor for the softmax function, and it's set empirically\n",
    "        # on a held-out data set. We're going to learn gamma's value by pretending it's\n",
    "        # a single 1 x 1 kernel.\n",
    "        weight = np.array([1]).reshape(1, 1, 1)\n",
    "        with_gamma = Convolution1D(1, 1, padding = \"same\", input_shape = (self.num_negatives + 1, 1), activation = \"linear\", use_bias = False, weights = [weight])(concat_Rs) # See equation (5).\n",
    "        with_gamma = Reshape((self.num_negatives + 1, ))(with_gamma)\n",
    "\n",
    "        # Finally, we use the softmax function to calculate P(D+|Q).\n",
    "        prob = Activation(\"softmax\")(with_gamma) # See equation (5).\n",
    "\n",
    "        # We now have everything we need to define our model.\n",
    "        self.model = Model(inputs = [query, pos_doc] + neg_docs, outputs = prob)\n",
    "        self.model.compile(optimizer = \"adadelta\", loss = \"categorical_crossentropy\")\n",
    "\n",
    "        self.encoder = Model(inputs=query, outputs=query_sem)\n",
    "\n",
    "    def batch_generator(self, reader, train_data, tokeniser, batch_size, max_len, nb_words):\n",
    "        while True:\n",
    "            for df in reader:\n",
    "                q = df.q.tolist()\n",
    "                if train_data in [\"1M_EN_QQ_log\", \"200_log\"]:\n",
    "                    d = [i.split(\"<sep>\")[0] for i in df.d.tolist()]\n",
    "                else:\n",
    "                    d = df.d.tolist()\n",
    "                \n",
    "                q = pad_sequences(tokeniser.texts_to_sequences(q), maxlen=max_len)\n",
    "                d = pad_sequences(tokeniser.texts_to_sequences(d), maxlen=max_len)\n",
    "                \n",
    "                q_one_hot = np.zeros((batch_size, nb_words))\n",
    "                for i in range(len(q)):\n",
    "                    q_one_hot[i][q[i]] = 1\n",
    "                    \n",
    "                d_one_hot = np.zeros((batch_size, nb_words))\n",
    "                for i in range(len(d)):\n",
    "                    d_one_hot[i][d[i]] = 1\n",
    "                    \n",
    "                    \n",
    "                # negative sampling from positive pool\n",
    "                neg_d_one_hot = [[] for j in range(self.num_negatives)]\n",
    "                for i in range(batch_size):\n",
    "                    possibilities = list(range(batch_size))\n",
    "                    possibilities.remove(i)\n",
    "                    negatives = np.random.choice(possibilities, self.num_negatives, replace = False)\n",
    "                    for j in range(self.num_negatives):\n",
    "                        negative = negatives[j]\n",
    "                        neg_d_one_hot[j].append(d_one_hot[negative].tolist())\n",
    "                \n",
    "                y = np.zeros((batch_size, self.num_negatives + 1))\n",
    "                y[:, 0] = 1\n",
    "                \n",
    "                for j in range(self.num_negatives):\n",
    "                    neg_d_one_hot[j] = np.array(neg_d_one_hot[j])\n",
    "                \n",
    "                \n",
    "                yield [q_one_hot, d_one_hot] + [neg_d_one_hot[j] for j in range(self.num_negatives)], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '10565' (I am process '25722')\n"
     ]
    }
   ],
   "source": [
    "run = VAE_BPE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.encoder.predict(np.random.randint(2, size=(10, 10))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 54.1447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15d3eca20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.model.fit(np.random.randint(2, size=(10, 10)), np.random.randint(2, size=(10, 10, 50005)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
