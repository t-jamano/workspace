{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Dense, Concatenate, Embedding, Merge, merge, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import objectives, backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import imdb\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "import pytrec_eval\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE = '/data/t-mipha/data/agi_encoder/v4/universal/CLICKED_QQ_EN_universal_train_1M.txt'\n",
    "df = pd.read_csv(TRAIN_DATA_FILE, usecols=[0,1], names=[\"q\", \"d\"], sep=\"\\t\", header=None, error_bad_lines=False)\n",
    "df = df.dropna()\n",
    "\n",
    "q_train = df.q.tolist()\n",
    "# use only first similar query\n",
    "d_train = [i.split(\"<sep>\")[0] for i in df.d.tolist()]\n",
    "y_train = np.ones(len(df))\n",
    "\n",
    "texts = q_train + d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir = '/data/t-mipha/data/query_similarity_ndcg/MayFlowerIdeal.txt'\n",
    "df_test = pd.read_csv(file_dir, names=[\"market\", \"qid\", \"q\", \"label\", \"d\", \"date\"], sep=\"\\t\", header=0, error_bad_lines=False)\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average query's length: 561829\n"
     ]
    }
   ],
   "source": [
    "print(\"Average query's length: {}\".format(np.max(list(map(len, texts)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 212 ms, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from l3wtransformer import L3wTransformer\n",
    "tokeniser = L3wTransformer()\n",
    "tokeniser.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1993708"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenisertexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train = tokeniser.texts_to_sequences(q_train)\n",
    "d_train = tokeniser.texts_to_sequences(d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average query's length: 23\n"
     ]
    }
   ],
   "source": [
    "max_len = np.mean(list(map(len, q_train + d_train)), dtype='int32')\n",
    "print(\"Average query's length: {}\".format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train = pad_sequences(q_train, maxlen=max_len)\n",
    "d_train = pad_sequences(d_train, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_test = pad_sequences(tokeniser.texts_to_sequences(df_test.q.tolist()), maxlen=max_len)\n",
    "d_test = pad_sequences(tokeniser.texts_to_sequences(df_test.d.tolist()), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = np.array([0 if i == \"Bad\" else 1 if i == \"Fare\" else 2 for i in df_test.label.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bad     9804\n",
       "Good    2719\n",
       "Fair    2609\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_2_trec(query, document, label, isQrel):\n",
    "    trec = {}\n",
    "    for i, j, k in zip(query, document, label):\n",
    "        if i not in trec:\n",
    "            trec[i] = {}\n",
    "        if j not in trec[i]:\n",
    "            trec[i][j] = {}\n",
    "        trec[i][j] = int(k) if isQrel else float(k)\n",
    "    return trec\n",
    "\n",
    "def evaluate(qrel, pred):\n",
    "\n",
    "    run = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "        qrel, {'map', 'ndcg'})\n",
    "\n",
    "    results = evaluator.evaluate(run)\n",
    "    print(\"NDCG: %f\" % np.mean([i['ndcg'] for i in results.values()]))\n",
    "    print(\"MAP: %f\" % np.mean([i['map'] for i in results.values()]))\n",
    "    \n",
    "qrel = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = tokeniser.max_ngrams + 5 + 1     # default l2wtransformer's max vocab = 50K + 5 for unknown word\n",
    "MAX_SEQUENCE_LENGTH = max_len\n",
    "EMBEDDING_DIM = 200 # similar to Bing pre-trained w2v\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, MAX_NB_WORDS))\n",
    "encoder = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, MAX_NB_WORDS))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(MAX_NB_WORDS, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 23, 50006)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(MAX_NB_WORDS, size=(10,MAX_SEQUENCE_LENGTH, MAX_NB_WORDS))\n",
    "model.predict([x, x]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self, max_len=10, emb_dim=100, nb_words=50000, lstm_dim=256):\n",
    "\n",
    "        q_input = Input(shape=(max_len,))\n",
    "        d_input = Input(shape=(max_len,))\n",
    "        \n",
    "        emb = Embedding(nb_words, emb_dim, mask_zero=True)\n",
    "\n",
    "        lstm = LSTM(lstm_dim)\n",
    "\n",
    "        self.q_embed = lstm(emb(q_input))\n",
    "        self.d_embed = lstm(emb(d_input))\n",
    "\n",
    "        concat = merge([self.q_embed, self.d_embed], mode=\"cos\")\n",
    "        \n",
    "        self.encoder = Model(q_input, self.q_embed)\n",
    "\n",
    "        pred = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "        self.model = Model(inputs=[q_input, d_input], outputs=pred)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = tokeniser.max_ngrams + 5 + 1     # default l2wtransformer's max vocab = 50K + 5 for unknown word\n",
    "MAX_SEQUENCE_LENGTH = max_len\n",
    "EMBEDDING_DIM = 200 # similar to Bing pre-trained w2v\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 1982s - loss: 0.1809 - acc: 0.9344\n",
      "NDCG: 0.529818\n",
      "MAP: 0.516537\n",
      "Epoch 1/1\n",
      " - 1978s - loss: 0.1523 - acc: 0.9442\n",
      "NDCG: 0.531364\n",
      "MAP: 0.518602\n",
      "Epoch 1/1\n",
      " - 1984s - loss: 0.1331 - acc: 0.9507\n",
      "NDCG: 0.531317\n",
      "MAP: 0.518825\n",
      "Epoch 1/1\n",
      " - 1997s - loss: 0.1181 - acc: 0.9558\n",
      "NDCG: 0.530491\n",
      "MAP: 0.517802\n"
     ]
    }
   ],
   "source": [
    "# lstm = LSTM_Model(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, MAX_NB_WORDS)\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    d_neg_train = np.copy(d_train)\n",
    "    np.random.shuffle(d_neg_train)\n",
    "    \n",
    "    q_ = np.concatenate([q_train, q_train])\n",
    "    d_ = np.concatenate([d_train, d_neg_train])\n",
    "    y_ = np.concatenate([np.ones(len(q_train)), np.zeros(len(q_train))])\n",
    "       \n",
    "    q, y = shuffle(q_, y_, random_state=0)\n",
    "    d, y = shuffle(d_, y_, random_state=0)\n",
    "       \n",
    "    \n",
    "    lstm.model.fit([q, d], y , batch_size=128, epochs=1, verbose=2)\n",
    "\n",
    "    pred = lstm.model.predict([q_test, d_test])\n",
    "    evaluate(qrel, pred)\n",
    "#     pred = get_cosine_sim(lstm.encoder.predict(q_test), lstm.encoder.predict(d_test))\n",
    "#     evaluate(qrel, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.529803\n",
      "MAP: 0.516425\n"
     ]
    }
   ],
   "source": [
    "#first epoch\n",
    "pred = get_cosine_sim(lstm.encoder.predict(q_test), lstm.encoder.predict(d_test))\n",
    "evaluate(qrel, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.478829\n",
      "MAP: 0.448108\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_cosine_sim(x, y):\n",
    "    tmp = []\n",
    "    for i,j in zip(x,y):\n",
    "        tmp.append(cosine_similarity(i.reshape(1, -1),j.reshape(1, -1)))\n",
    "    return np.array(tmp).flatten()\n",
    "\n",
    "def auc(y_test, pred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, pred, pos_label=1)\n",
    "    return metrics.auc(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.506189\n",
      "MAP: 0.483753\n"
     ]
    }
   ],
   "source": [
    "pred = get_cosine_sim(q_test, d_test)\n",
    "run = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "    qrel, {'map', 'ndcg'})\n",
    "\n",
    "results = evaluator.evaluate(run)\n",
    "print(\"NDCG: %f\" % np.mean([i['ndcg'] for i in results.values()]))\n",
    "print(\"MAP: %f\" % np.mean([i['map'] for i in results.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 12.8 s, total: 40.3 s\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "with open(r\"/data/t-mipha/data/agi_encoder/v4/universal/embedding_dict.pkl\", \"rb\") as input_file:\n",
    "    e = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10923 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = len(e['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "w2v = Sequential()\n",
    "w2v.add(embedding_layer)\n",
    "w2v.add(GlobalAveragePooling1D())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_test = pad_sequences(tokenizer.texts_to_sequences(df_test.q.tolist()), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "d_test = pad_sequences(tokenizer.texts_to_sequences(df_test.d.tolist()), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.522923\n",
      "MAP: 0.507142\n"
     ]
    }
   ],
   "source": [
    "w2v = Sequential()\n",
    "w2v.add(embedding_layer)\n",
    "w2v.add(GlobalAveragePooling1D())\n",
    "\n",
    "pred = get_cosine_sim(w2v.predict(q_test), w2v.predict(d_test))\n",
    "run = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "\n",
    "results = evaluator.evaluate(run)\n",
    "print(\"NDCG: %f\" % np.mean([i['ndcg'] for i in results.values()]))\n",
    "print(\"MAP: %f\" % np.mean([i['map'] for i in results.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827361183641\n",
      "0.745250255879\n"
     ]
    }
   ],
   "source": [
    "w2v = Sequential()\n",
    "w2v.add(embedding_layer)\n",
    "w2v.add(GlobalMaxPooling1D())\n",
    "\n",
    "pred = get_cosine_sim(w2v.predict(q_test), w2v.predict(d_test))\n",
    "run = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "\n",
    "results2 = evaluator.evaluate(run)\n",
    "print(np.mean([i['ndcg'] for i in results2.values()]))\n",
    "print(np.mean([i['map'] for i in results2.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def ttest(res1, res2, metric=\"ndcg\"):\n",
    "    res1 = [i[metric] for i in res1.values()]\n",
    "    res2 = [i[metric] for i in res2.values()]\n",
    "    print(scipy.stats.ttest_rel(res1, res2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
