{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Models import *\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10/10 [==============================] - 2s 244ms/step - loss: 48.0443 - recon_q_loss: 23.0795 - recon_d_loss: 23.0928 - cos_qd_loss: 1.8720\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/work/data/bpe/en.wiki.bpe.op50000.model')\n",
    "bpe = KeyedVectors.load_word2vec_format(\"/work/data/bpe/en.wiki.bpe.op50000.d200.w2v.bin\", binary=True)\n",
    "\n",
    "run = VarAutoEncoderQD(100, 10, bpe.get_keras_embedding(True), [100,10])\n",
    "x = np.random.randint(2, size=(10,10))\n",
    "y = np.random.randint(2, size=(10,10,100))\n",
    "y2 = np.random.randint(2, size=(10))\n",
    "a = run.model.fit([x,x], [y,y,y2], epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\r",
      "10/10 [==============================] - 0s 16ms/step - loss: 47.9076 - recon_q_loss: 23.0650 - recon_d_loss: 23.0678 - cos_qd_loss: 1.7749\n"
     ]
    }
   ],
   "source": [
    "a = run.model.fit([x,x], [y,y,y2], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tbCallBack = TensorBoard(log_dir='/work/data/logs/aaaa', histogram_freq=0, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Adam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.907630920410156]\n",
      "[23.064964294433594]\n",
      "[23.067764282226562]\n",
      "[1.7749018669128418]\n"
     ]
    }
   ],
   "source": [
    "for k in a.history:\n",
    "    print(a.history[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss = 47.9076, recon_q_loss = 23.0650, recon_d_loss = 23.0678, cos_qd_loss = 1.7749'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([ \"%s = %.4f\" % (k, a.history[k][-1]) for k in a.history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'loss': [47.907630920410156], 'recon_q_loss': [23.064964294433594], 'recon_d_loss': [23.067764282226562], 'cos_qd_loss': [1.7749018669128418]}\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(a.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 46.1956 - recon_q_loss: 22.9992 - recon_d_loss: 22.9842 - cos_qd_loss: 0.2122\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 46.3217 - recon_q_loss: 22.9914 - recon_d_loss: 22.9922 - cos_qd_loss: 0.3381\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 46.2104 - recon_q_loss: 22.9940 - recon_d_loss: 22.9765 - cos_qd_loss: 0.2399\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 46.3301 - recon_q_loss: 22.9731 - recon_d_loss: 22.9778 - cos_qd_loss: 0.3792\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 46.2832 - recon_q_loss: 22.9945 - recon_d_loss: 22.9739 - cos_qd_loss: 0.3147\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "a = run.model.fit([x,x], [y,y,y2], epochs=5, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LETTER_GRAM_SIZE = 3 # See section 3.2.\n",
    "WINDOW_SIZE = 3 # See section 3.2.\n",
    "TOTAL_LETTER_GRAMS = 5005\n",
    "WORD_DEPTH = WINDOW_SIZE * TOTAL_LETTER_GRAMS # See equation (1).\n",
    "K = 300 # Dimensionality of the max-pooling layer. See section 3.4.\n",
    "L = 128 # Dimensionality of latent semantic space. See section 3.5.\n",
    "J = 1 # Number of random unclicked documents serving as negative examples for a query. See section 4.\n",
    "FILTER_LENGTH = 1 # We only consider one time step for convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dssm = DSSM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pre-trained trigram\n",
    "tokeniser = L3wTransformer(TOTAL_LETTER_GRAMS)\n",
    "tokeniser = tokeniser.load(\"/work/data/trigram/2M_50k_trigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_words = 50005\n",
    "max_len = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 19898: expected 6 fields, saw 8\\nSkipping line 20620: expected 6 fields, saw 8\\nSkipping line 38039: expected 6 fields, saw 8\\n'\n"
     ]
    }
   ],
   "source": [
    "df_may, qrel_may = get_test_data(\"MayFlower\")\n",
    "df_june, qrel_june = get_test_data(\"JuneFlower\")\n",
    "\n",
    "q_may = to_2D_one_hot(parse_texts(df_may.q.tolist(), tokeniser, max_len), nb_words)\n",
    "d_may = to_2D_one_hot(parse_texts(df_may.d.tolist(), tokeniser, max_len), nb_words)\n",
    "\n",
    "\n",
    "q_june = to_2D_one_hot(parse_texts(df_june.q.tolist(), tokeniser, max_len), nb_words)\n",
    "d_june = to_2D_one_hot(parse_texts(df_june.d.tolist(), tokeniser, max_len), nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 627s - loss: 0.3930\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/workspace/Models.py:26: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  pred = merge([q_input, d_input], mode=\"cos\")\n",
      "/home/t-jamano/.local/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.525413\n",
      "MAP: 0.510676\n",
      "NDCG: 0.810011\n",
      "MAP: 0.726695\n"
     ]
    }
   ],
   "source": [
    "file_dir = '/data/t-mipha/data/agi_encoder/v4/universal/CLICKED_QQ_EN_universal_train_1M.txt'\n",
    "\n",
    "reader = pd.read_csv(file_dir, chunksize=batch_size, iterator=True, usecols=[0,1], names=[\"q\", \"d\"], sep=\"\\t\", header=None, error_bad_lines=False)\n",
    "\n",
    "def qq_batch_generator(reader, tokeniser, batch_size, max_len, nb_words):\n",
    "    for df in reader:\n",
    "        q = df.q.tolist()\n",
    "        d = [i.split(\"<sep>\")[0] for i in df.d.tolist()]\n",
    "        \n",
    "        q = pad_sequences(tokeniser.texts_to_sequences(q), maxlen=max_len)\n",
    "        d = pad_sequences(tokeniser.texts_to_sequences(d), maxlen=max_len)\n",
    "        \n",
    "        q_one_hot = np.zeros((batch_size, nb_words))\n",
    "        for i in range(len(q)):\n",
    "            q_one_hot[i][q[i]] = 1\n",
    "            \n",
    "        d_one_hot = np.zeros((batch_size, nb_words))\n",
    "        for i in range(len(d)):\n",
    "            d_one_hot[i][d[i]] = 1\n",
    "            \n",
    "            \n",
    "        # negative sampling from positive pool\n",
    "        neg_d_one_hot = [[] for j in range(J)]\n",
    "        for i in range(batch_size):\n",
    "            possibilities = list(range(batch_size))\n",
    "            possibilities.remove(i)\n",
    "            negatives = np.random.choice(possibilities, J, replace = False)\n",
    "            for j in range(J):\n",
    "                negative = negatives[j]\n",
    "                neg_d_one_hot[j].append(d_one_hot[negative].tolist())\n",
    "        \n",
    "        y = np.zeros((batch_size, J + 1))\n",
    "        y[:, 0] = 1\n",
    "        \n",
    "        for j in range(J):\n",
    "            neg_d_one_hot[j] = np.array(neg_d_one_hot[j])\n",
    "        \n",
    "#         print(q_one_hot.shape, d_one_hot.shape, len(neg_d_one_hot))\n",
    "#         print(neg_d_one_hot[0])\n",
    "\n",
    "        # negative sampling from randomness\n",
    "        # for j in range(J):\n",
    "        #     neg_d_one_hot[j] = np.random.randint(2, size=(batch_size, 10, WORD_DEPTH))\n",
    "        \n",
    "\n",
    "#         q_one_hot = to_categorical(q, nb_words)   \n",
    "#         q_one_hot = q_one_hot.reshape(batch_size, max_len, nb_words)\n",
    "        \n",
    "        \n",
    "        yield [q_one_hot, d_one_hot] + [neg_d_one_hot[j] for j in range(J)], y\n",
    "\n",
    "        \n",
    "dssm.model.fit_generator(qq_batch_generator(reader, tokeniser, batch_size, max_len, nb_words), steps_per_epoch=1000, epochs=1, verbose=2, callbacks=[TQDMNotebookCallback()])       \n",
    "cosine = CosineSim(L)\n",
    "\n",
    "\n",
    "for q, d, qrel, df in [[q_may, d_may, qrel_may, df_may], [q_june, d_june, qrel_june, df_june]]:\n",
    "    pred = cosine.model.predict([dssm.encoder.predict(q), dssm.encoder.predict(d)])\n",
    "    pred = convert_2_trec(df.q.tolist(), df.d.tolist(), pred, False)\n",
    "    evaluate(qrel, pred)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/work/data/train_data/30M_EN_pos_qd_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_dir, usecols=[0,1], names=[\"q\", \"d\"], sep=\"\\t\", header=None, error_bad_lines=False)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir = '/data/chzho/deepqts/train_data/unifiedclick/join_oneyearsample_2B_training_all_top10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 11s, sys: 4min 14s, total: 21min 26s\n",
      "Wall time: 36min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(file_dir, nrows=1000000000, usecols=[0,1,3,5], names=[\"label\", \"q\", \"d\", \"market\"], sep=\"\\t\", header=None, error_bad_lines=False)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[(df.market.str.contains(\"en-\")) & (df.label == 1)].to_csv(\"/work/data/train_data/EN_QD_log\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/work/data/train_data/1M_EN_qq_log\", usecols=[0,1], names=[\"q\", \"d\"], sep=\"\\t\", header=None, error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 47s, sys: 0 ns, total: 36min 47s\n",
      "Wall time: 46min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "file = open(\"/work/data/train_data/100M_QD.txt\", \"a+\") \n",
    "with open (\"/data/chzho/deepqts/train_data/unifiedclick/join_oneyearsample_2B_training_all_top10\", \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        s = line.split(\"\\t\")\n",
    "        market = s[5]\n",
    "        if \"en-\" not in market:\n",
    "            continue\n",
    "            \n",
    "        file.write(\"%s\\t%s\\t%s\\n\" % (s[0], s[1], s[3]))\n",
    "        \n",
    "        count = count + 1\n",
    "        if count == 100000000:\n",
    "            break\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
