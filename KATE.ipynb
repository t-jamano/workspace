{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Models import *\n",
    "from Utils import *\n",
    "from FastModels import *\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/work/data/bpe/en.wiki.bpe.op50000.model')\n",
    "bpe = KeyedVectors.load_word2vec_format(\"/work/data/bpe/en.wiki.bpe.op50000.d200.w2v.bin\", binary=True)\n",
    "bpe2 = KeyedVectors.load_word2vec_format(\"/work/data/bpe/en.wiki.bpe.op50000.d200.w2v.bin\", binary=True)\n",
    "bpe_dict = { bpe.index2word[i]: i for i in range(len(bpe.index2word))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "enablePadding = True\n",
    "nb_words= len(bpe_dict)\n",
    "\n",
    "df_may, qrel_may = get_test_data(\"MayFlower\", \"/work/\")\n",
    "df_june, qrel_june = get_test_data(\"JuneFlower\", \"/work/\")\n",
    "df_july, qrel_july = get_test_data(\"JulyFlower\", \"/work/\")\n",
    "\n",
    "q_may = parse_texts_bpe(df_may.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_may = parse_texts_bpe(df_may.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "q_june = parse_texts_bpe(df_june.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_june = parse_texts_bpe(df_june.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "q_july = parse_texts_bpe(df_july.q.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "d_july = parse_texts_bpe(df_july.d.tolist(), sp, bpe_dict, max_len, enablePadding)\n",
    "\n",
    "test_set = [[q_may, d_may, qrel_may, df_may, \"MayFlower\"], [q_june, d_june, qrel_june, df_june, \"JuneFlower\"], [q_july, d_july, qrel_july, df_july, \"JulyFlower\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_train_data(corpus, max_len, padding=\"pre\"):\n",
    "    x = []\n",
    "    y = []\n",
    "    for query in corpus:\n",
    "        for j in range(len(query)-1):\n",
    "            x.append(query[:j+1])\n",
    "            y.append(query[j+1])\n",
    "            \n",
    "    x = pad_sequences(x, maxlen=max_len, padding=padding)\n",
    "    y = np.array(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_q_july, y_q_july = create_train_data(parse_texts_bpe(df_july.q.tolist(), sp, bpe_dict, max_len, False), max_len)\n",
    "x_d_july, y_d_july = create_train_data(parse_texts_bpe(df_july.d.tolist(), sp, bpe_dict, max_len, False), max_len)\n",
    "x_q_df, y_q_df = create_train_data(parse_texts_bpe(df.q.tolist(), sp, bpe_dict, max_len, False), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/work/data/train_data/QueryQueryLog\", nrows=1000, names=[\"q\", \"d\", \"label\"], sep=\"\\t\", header=None, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _q_july = pad_sequences(pad_sequences(q_july, maxlen=max_len+1, padding='pre'), maxlen=max_len, truncating=\"post\")\n",
    "# _d_july = pad_sequences(pad_sequences(d_july, maxlen=max_len+1, padding='pre'), maxlen=max_len, truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_df = parse_texts_bpe(df.q.tolist(), sp, bpe_dict, max_len, enablePadding=enablePadding)\n",
    "d_df = parse_texts_bpe(df.d.tolist(), sp, bpe_dict, max_len, enablePadding=enablePadding)\n",
    "# q_df_one_hot = to_categorical(q_df, nb_words)\n",
    "# q_df_one_hot = q_df_one_hot.reshape(len(q_df), max_len, nb_words)\n",
    "# _q_df = pad_sequences(pad_sequences(q_df, maxlen=max_len+1, padding='pre'), maxlen=max_len, truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = q_july\n",
    "x = to_categorical(q, nb_words)\n",
    "x = x.reshape(len(q_july), max_len, nb_words)\n",
    "\n",
    "d = d_july\n",
    "y = to_categorical(d, nb_words)\n",
    "y = y.reshape(len(d_july), max_len, nb_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_q_july = pad_sequences(pad_sequences(q_july, maxlen=max_len+1, padding='post'), maxlen=max_len, truncating=\"pre\")\n",
    "_d_july = pad_sequences(pad_sequences(d_july, maxlen=max_len+1, padding='post'), maxlen=max_len, truncating=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_q_df = pad_sequences(pad_sequences(q_df, maxlen=max_len+1, padding='post'), maxlen=max_len, truncating=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_train = np.concatenate([q_july, d_july])\n",
    "# y_train = np.concatenate([np.expand_dims(q_july, axis=-1), np.expand_dims(d_july, axis=-1)])\n",
    "\n",
    "x_train = q_july\n",
    "y_train = np.expand_dims(q_july, axis=-1)\n",
    "y_train = np.expand_dims(_q_july, axis=-1)\n",
    "\n",
    "# y_train = np.concatenate([np.expand_dims(_q_july, axis=-1), np.expand_dims(_d_july, axis=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = q_df\n",
    "# y_train = np.expand_dims(q_df, axis=-1)\n",
    "y_train = np.expand_dims(_q_df, axis=-1)\n",
    "# y_train = q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "w2v = Sequential()\n",
    "w2v.add(bpe.get_keras_embedding(train_embeddings=True))\n",
    "w2v.add(GlobalMaxPooling1D())\n",
    "\n",
    "# pred = get_cosine_sim(w2v.predict(q_july), w2v.predict(d_july))\n",
    "# print(auc(qrel_july, pred.flatten()))\n",
    "\n",
    "\n",
    "def eval(epoch, logs):\n",
    "    \n",
    "    loss = logs.get(\"loss\")\n",
    "    val_loss = logs.get(\"val_loss\")\n",
    "    \n",
    "#     may_ndcg, june_ndcg, july_auc = evaluate(run, cosine, test_set)\n",
    "#     print_output = '%s, Epoch %d, [%.1f s], May = %.4f, June = %.4f, July = %.4f, Loss = %.4f, V_Loss = %.4f \\n' % (run.name(), epoch, 0, may_ndcg, june_ndcg, july_auc, loss, val_loss)\n",
    "#     print(print_output)\n",
    "    \n",
    "#     generate_output(run.model.predict(q_july), run.model.predict(d_july))\n",
    "\n",
    "    if hasattr(run, 'generator'):\n",
    "        generate_output(run.generator.predict(q_july), run.generator.predict(d_july))\n",
    "    elif \"seqvae\" in run.name():\n",
    "\n",
    "        generate_output(run.model.predict([q_july, _q_july]), run.model.predict([d_july, _d_july]))\n",
    "    elif \"kate_lstm\" in run.name() or \"vae_lstm\" in run.name():\n",
    "#         sample new query\n",
    "#         generate_output(run.model.predict(q_july)[0], run.model.predict(d_july)[0])\n",
    "        generate_output(run.decoder.predict(run.encoder.predict(q_july)), run.decoder.predict(run.encoder.predict(d_july)))\n",
    "    elif \"naive_vae\" in run.name() or \"nvdm\" in run.name():\n",
    "        print(\"no generated query\")\n",
    "    else:\n",
    "        generate_output(run.model.predict(q_july), run.model.predict(d_july))\n",
    "    \n",
    "    \n",
    "    #       combine with w2v\n",
    "    if run.name() == \"nvdm\":\n",
    "        q = run.encoder\n",
    "#     q = np.concatenate([w2v.predict(q_july), run.encoder.predict(q_july)], axis=-1)\n",
    "#     d = np.concatenate([w2v.predict(d_july), run.encoder.predict(d_july)], axis=-1)\n",
    "    \n",
    "    cosine = CosineSim(q.shape[-1])\n",
    "    \n",
    "    pred = cosine.model.predict([q, d])\n",
    "    print(loss, val_loss, auc(qrel_july, pred.flatten()))\n",
    "    \n",
    "    \n",
    "class QueryGenerator():\n",
    "    \n",
    "    def __init__(self, input_size, max_len, emb, dim, comp_topk=None, ctype=None, epsilon_std=1.0, optimizer=Adadelta(lr=2.), kl_weight=0, cos_weight=1):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.dim = dim\n",
    "        self.emb = emb\n",
    "        self.comp_topk = comp_topk\n",
    "        self.ctype = ctype\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.max_len = max_len\n",
    "        self.nb_words = input_size\n",
    "        self.optimizer = optimizer\n",
    "        self.kl_weight = kl_weight\n",
    "        self.cos_weight = cos_weight\n",
    "        \n",
    "        self.build()\n",
    "    def name(self):\n",
    "        return \"lstm\"\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        emb = self.emb\n",
    "        emb.masking = True\n",
    "        lstm = LSTM(self.dim[1], return_sequences=True)\n",
    "        dense = Dense(self.nb_words)\n",
    "        \n",
    "        self.model = Sequential([emb, lstm, dense, Activation('softmax')])\n",
    "        self.encoder = Sequential([emb, lstm, GlobalMaxPooling1D()])\n",
    "        \n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KL_Annealing(Callback):\n",
    "    \n",
    "    def __init__(self, run):\n",
    "        super(KL_Annealing, self).__init__()\n",
    "        \n",
    "        self.run = run\n",
    "        self.kl_inc_rate = 1 / 5000. # set the annealing rate for KL loss\n",
    "        self.cos_inc_rate = 1\n",
    "        self.max_cos_weight = 150.\n",
    "        self.max_kl_weight = 1.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if hasattr(self.run, 'loss_layer'):\n",
    "            self.run.loss_layer.kl_weight = min(self.run.loss_layer.kl_weight + self.kl_inc_rate, self.max_kl_weight)\n",
    "        elif hasattr(self.run, \"kl_weight\"):\n",
    "            self.run.kl_weight = min(self.run.kl_weight + self.kl_inc_rate, self.max_kl_weight)\n",
    "            \n",
    "            if \"kate_lstm\" in self.run.name() or \"vae_lstm\" in self.run.name():\n",
    "                self.run.model.compile(optimizer=self.run.optimizer, loss=[\"sparse_categorical_crossentropy\", self.run.kl_loss])\n",
    "            else:\n",
    "                self.run.model.compile(optimizer=self.run.optimizer, loss=[self.run.vae_loss])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "class SeqVAE():\n",
    "    \n",
    "    def __init__(self, nb_words, max_len, emb, dim, comp_topk=None, ctype=None, epsilon_std=1.0, mode=0, optimizer=Adadelta(lr=2.), enableGAN=False):\n",
    "        \n",
    "        \n",
    "        self.dim = dim\n",
    "        self.comp_topk = comp_topk\n",
    "        self.ctype = ctype\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.mode = mode\n",
    "        self.nb_words = nb_words\n",
    "        self.max_len = max_len\n",
    "        self.emb = emb\n",
    "        self.enableGAN = enableGAN\n",
    "        self.kl_weight = 0\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.build()\n",
    "    def name(self):\n",
    "        return \"seqvae%d\" % self.mode\n",
    "    \n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.dim[0]), mean=0.,\\\n",
    "                                  stddev=self.epsilon_std)\n",
    "\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    \n",
    "    def vae_loss(self, x, x_):\n",
    "        \n",
    "        rec_loss =  K.sum(K.sparse_categorical_crossentropy(x, x_), axis=-1)\n",
    "\n",
    "        if self.mode == 0:\n",
    "            kl_loss = - 0.5 * K.sum(1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var), axis=-1)\n",
    "            return rec_loss + (self.kl_weight * kl_loss)\n",
    "        elif self.mode == 1:\n",
    "            kl_h_loss = - 0.5 * K.sum(1 + self.h_var - K.square(self.h_mean) - K.exp(self.h_var), axis=-1)\n",
    "            kl_c_loss = - 0.5 * K.sum(1 + self.c_var - K.square(self.c_mean) - K.exp(self.c_var), axis=-1)\n",
    "            return rec_loss + (self.kl_weight * (kl_h_loss + hl_c_loss))\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        encoder_inputs = Input(shape=(self.max_len,))\n",
    "        encoder = LSTM(self.dim[0], return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder(self.emb(encoder_inputs))\n",
    "        \n",
    "        if self.mode == 0:\n",
    "            self.z_mean = Dense(self.dim[0])(encoder_outputs)\n",
    "            self.z_log_var = Dense(self.dim[0])(encoder_outputs)\n",
    "\n",
    "            z = Lambda(self.sampling, output_shape=(self.dim[0],))([self.z_mean, self.z_log_var])\n",
    "            encoder_states = [z, z]\n",
    "        elif self.mode == 1:\n",
    "            self.h_mean = Dense(self.dim[0])(state_h)\n",
    "            self.h_var = Dense(self.dim[0])(state_h)\n",
    "            \n",
    "            self.c_mean = Dense(self.dim[0])(state_c)\n",
    "            self.c_var = Dense(self.dim[0])(state_c)\n",
    "\n",
    "            z_h = Lambda(self.sampling, output_shape=(self.dim[0],))([self.h_mean, self.h_var])\n",
    "            z_c = Lambda(self.sampling, output_shape=(self.dim[0],))([self.c_mean, self.c_var])\n",
    "\n",
    "            encoder_states = [z_h, z_c]\n",
    "        \n",
    "        # Set up the decoder, using `encoder_states` as initial state.\n",
    "        decoder_inputs = Input(shape=(self.max_len,))\n",
    "        # We set up our decoder to return full output sequences,\n",
    "        # and to return internal states as well. We don't use the\n",
    "        # return states in the training model, but we will use them in inference.\n",
    "        decoder_lstm = LSTM(self.dim[0], return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(self.emb(decoder_inputs), initial_state=encoder_states)\n",
    "        decoder_dense = Dense(self.nb_words, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        # Define the model that will turn\n",
    "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "        self.model = Model([encoder_inputs, decoder_inputs], [decoder_outputs])\n",
    "#         self.model.compile(optimizer=self.optimizer, loss=[\"sparse_categorical_crossentropy\"])\n",
    "        self.model.compile(optimizer=self.optimizer, loss=[self.vae_loss])\n",
    "\n",
    "        self.encoder = Model(encoder_inputs, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run = SeqVAE(nb_words, max_len, bpe.get_keras_embedding(train_embeddings=True), [300, 128], mode=0, optimizer=RMSprop(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Data\n",
    "_q_july = pad_sequences(pad_sequences(q_july, maxlen=max_len+1, padding='pre'), maxlen=max_len, truncating=\"post\")\n",
    "_d_july = pad_sequences(pad_sequences(d_july, maxlen=max_len+1, padding='pre'), maxlen=max_len, truncating=\"post\")\n",
    "\n",
    "x_train = [q_july, _q_july]\n",
    "y_train = np.expand_dims(q_july, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run = VAE_LSTM(nb_words, max_len, bpe.get_keras_embedding(train_embeddings=True), [300, 128], kl_weight=0.2, optimizer=RMSprop(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7968/|/[loss: 6.749, decoded_mean_loss_1: 4.696, decoded_mean_loss_2: 2.054] 100%|| 7968/8000 [00:49<00:00, 322.81it/s]h ives causes        : abbey voc hara hara hara hara hara hara hara activation\n",
      "\n",
      "difference between whe y protein and whe y protein isolate : abbey fresno sharp sharp sharp sharp sharp sharp sharp activation\n",
      "\n",
      "average cost of ky bella      : back activation activation activation activation activation activation activation activation activation\n",
      "\n",
      "mac ular hole surgery recovery      : abbey abbey activation activation activation activation activation activation activation activation\n",
      "\n",
      "kir stin powers        :          ww\n",
      "\n",
      "top capital gains tax rate  2 0 <unk>  :   to to to to to to to to\n",
      "\n",
      " per oid ic table      :  mov mov mov mov mov mov mov mov mov\n",
      "\n",
      "6.742331712722779 5.346508201599121 0.5740621316907203\n",
      "h ives causes        : tar tar smoke smoke smoke smoke smoke tar tar ve\n",
      "\n",
      "difference between whe y protein and whe y protein isolate : tar tar tar tar tar tar tar tar tar tar\n",
      "\n",
      "average cost of ky bella      :   house house house house house house house house\n",
      "\n",
      "mac ular hole surgery recovery      : tar ek ek ek ek ek ek ek ek ek\n",
      "\n",
      "kir stin powers        :  dec dec dec dec dec dec dec dec dec\n",
      "\n",
      "top capital gains tax rate  2 0 <unk>  : ek ek ek ek ek ek ek ek ek ek\n",
      "\n",
      " per oid ic table      : trips ek ek ek ek ek ek ek ek ek\n",
      "\n",
      "7968/|/[loss: 5.030, decoded_mean_loss_1: 4.232, decoded_mean_loss_2: 0.799] 100%|| 7968/8000 [00:39<00:00, 310.18it/s]5.029484869003296 7.185558666229248 0.5506682381195935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f55cd8d9ca65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambdaCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKL_Annealing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTQDMNotebookCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/t-jamano/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/t-jamano/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/t-jamano/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 run_metadata):\n\u001b[1;32m   1016\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1066\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train = q_july\n",
    "y_train = np.expand_dims(q_july, axis=-1)\n",
    "x_train = q_df\n",
    "y_train = np.expand_dims(q_df, axis=-1)\n",
    "\n",
    "hist = run.model.fit(x_train, [y_train, y_train], epochs=50, verbose=0, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(verbose=1, patience=5),ReduceLROnPlateau(verbose=1, patience=3), LambdaCallback(on_epoch_end=eval), KL_Annealing(run), TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QueryGenerator(object):\n",
    "    \n",
    "    def __init__(self, nb_words, max_len, emb, dim, comp_topk=None, ctype=None, epsilon_std=1.0, optimizer=Adadelta(lr=2.), enableGAN=False, kl_weight=0):\n",
    "        self.dim = dim\n",
    "        self.comp_topk = comp_topk\n",
    "        self.ctype = ctype\n",
    "        self.epsilon_std = epsilon_std\n",
    "\n",
    "        self.nb_words = nb_words\n",
    "        self.max_len = max_len\n",
    "        self.emb = emb\n",
    "        self.enableGAN = enableGAN\n",
    "        self.optimizer = optimizer\n",
    "        self.kl_weight = kl_weight\n",
    "        \n",
    "        act = 'tanh'\n",
    "        input_layer = Input(shape=(self.max_len,))\n",
    "        embedding = emb\n",
    "        embedding.mask_zero = True\n",
    "        \n",
    "        # self.embed_layer.trainable = False\n",
    "        bilstm = Bidirectional(LSTM(self.dim[0], return_sequences=True))\n",
    "#         bilstm = LSTM(self.dim[0])\n",
    "        dense = Dense(self.nb_words, activation=\"softmax\")\n",
    "        \n",
    "        pred = dense(bilstm(embedding(input_layer)))\n",
    "        \n",
    "        self.model = Model(input_layer, pred)\n",
    "        # Dropout(0.2),\n",
    "#         self.model = Sequential([embedding, bilstm,  dense])\n",
    "        self.model.compile(optimizer=self.optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, nb_words, max_len, emb, emb2, dim, comp_topk=None, ctype=None, epsilon_std=1.0, optimizer=RMSprop(lr=0.01), enableGAN=False, kl_weight=0):\n",
    "        self.dim = dim\n",
    "        self.comp_topk = comp_topk\n",
    "        self.ctype = ctype\n",
    "        self.epsilon_std = epsilon_std\n",
    "\n",
    "        self.nb_words = nb_words\n",
    "        self.max_len = max_len\n",
    "        self.emb = emb\n",
    "        self.emb2 = emb2\n",
    "        self.enableGAN = enableGAN\n",
    "        self.optimizer = optimizer\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator, self.encoder = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.max_len,))\n",
    "        text = self.generator(z)\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(text)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.model = Model(z, validity)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "        \n",
    "        emb = self.emb\n",
    "        print(emb)\n",
    "        lstm = LSTM(self.dim[1], return_sequences=True)\n",
    "        dense = Dense(self.nb_words)\n",
    "        \n",
    "        generator = Sequential([emb, lstm, dense, Activation('softmax')])\n",
    "        encoder = Sequential([emb, lstm, GlobalMaxPooling1D()])\n",
    "        \n",
    "        generator.compile(optimizer=self.optimizer, loss=[\"sparse_categorical_crossentropy\"])\n",
    "\n",
    "        return generator, encoder\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \n",
    "        input_layer = Input(shape=(self.max_len, self.nb_words,))\n",
    "        emb = OnehotEmbedding(200, d=2)\n",
    "        lstm = LSTM(self.dim[1])\n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "        h = emb(input_layer)\n",
    "        h = lstm(h)\n",
    "        pred = dense(h)\n",
    "\n",
    "        discriminator = Model(input_layer, pred)\n",
    "        emb.set_weights([bpe2.vectors])\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    \n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.embeddings.Embedding object at 0x7fae45412518>\n"
     ]
    }
   ],
   "source": [
    "run = GAN(nb_words, max_len, bpe.get_keras_embedding(train_embeddings=True), bpe2.vectors, [300, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.748254, acc.: 49.77%] [G loss: 0.520398] AUC: 0.4191\n",
      "1 [D loss: 0.709304, acc.: 49.77%] [G loss: 0.607252] AUC: 0.4221\n",
      "2 [D loss: 0.686104, acc.: 49.77%] [G loss: 0.694397] AUC: 0.4150\n",
      "3 [D loss: 0.677233, acc.: 82.75%] [G loss: 0.768257] AUC: 0.4199\n",
      "4 [D loss: 0.719529, acc.: 35.33%] [G loss: 0.731035] AUC: 0.3982\n"
     ]
    }
   ],
   "source": [
    "# Adversarial ground truths\n",
    "valid = np.ones((len(q_july), 1))\n",
    "fake = np.zeros((len(q_july), 1))\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random batch of images\n",
    "    imgs = x\n",
    "\n",
    "    # Generate a batch of new images\n",
    "    gen_imgs = run.generator.predict(d_july)\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = run.discriminator.train_on_batch(imgs, valid)\n",
    "    d_loss_fake = run.discriminator.train_on_batch(gen_imgs, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "#     noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "#     # Train the generator (to have the discriminator label samples as valid)\n",
    "    g_loss = run.model.train_on_batch(d_july, valid)\n",
    "\n",
    "#     # Plot the progress\n",
    "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] AUC: %.4f\" % (epoch, d_loss[0], 100*d_loss[1], g_loss, gan_eval()))\n",
    "#     generate_output(run.generator.predict(q_july), run.generator.predict(d_july))\n",
    "# For the combined model we will only train the generator\n",
    "#     # If at save interval => save generated image samples\n",
    "#     if epoch % sample_interval == 0:\n",
    "#         self.sample_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NVDM():\n",
    "    def __init__(self, mode=0):\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "        dim = [300,200]\n",
    "        X = Input(shape=(nb_words,))\n",
    "        x = Input(shape=(max_len,))\n",
    "        enc_emb = bpe.get_keras_embedding(True)\n",
    "        h1 = Dense(dim[0], activation=\"relu\")\n",
    "        h2 = Dense(dim[0], activation=\"relu\")\n",
    "        mean = Dense(dim[1])\n",
    "        log_var = Dense(dim[1])\n",
    "\n",
    "        if self.mode == 0:\n",
    "            z_mean = mean(h2(h1(X)))\n",
    "            z_log_var = log_var(h2(h1(X)))\n",
    "        elif self.mode == 1:\n",
    "            z_mean = mean(h2(h1(enc_emb(x))))\n",
    "            z_log_var = log_var(h2(h1(enc_emb(x))))\n",
    "\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            if self.mode == 0:\n",
    "                epsilon = K.random_normal(shape=(K.shape(z_mean)[0], dim[1]), mean=0.,\\\n",
    "                                          stddev=1)\n",
    "            elif self.mode == 1:\n",
    "                epsilon = K.random_normal(shape=(K.shape(z_mean)[0], max_len, dim[1]), mean=0.,\\\n",
    "                                                          stddev=1)\n",
    "            return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "        \n",
    "        if self.mode == 0:\n",
    "            z = Lambda(sampling, output_shape=(dim[1],))([z_mean, z_log_var])\n",
    "            repeated_z = RepeatVector(max_len)(z)\n",
    "        elif self.mode == 1:\n",
    "            z = Lambda(sampling, output_shape=(max_len, dim[1],))([z_mean, z_log_var])\n",
    "            repeated_z = z\n",
    "\n",
    "        # Decoder\n",
    "        \n",
    "        \n",
    "\n",
    "        dec_emb = bpe2.get_keras_embedding(True)\n",
    "        dec_bias = Embedding(nb_words, 200, embeddings_initializer=\"zeros\")\n",
    "\n",
    "        Rx = dec_emb(x)\n",
    "        bx = dec_bias(x)\n",
    "\n",
    "        _X = merge([merge([repeated_z, Rx], mode=\"mul\"), bx], mode=\"sum\")\n",
    "        _X = Dense(nb_words, activation=\"softmax\")(_X)\n",
    "        \n",
    "        if self.mode == 0:\n",
    "            self.model = Model([X, x], [_X, _X])\n",
    "            self.encoder = Model([X], z_mean)\n",
    "        elif self.mode == 1:\n",
    "            self.model = Model(x, [_X, _X])\n",
    "            self.encoder = Model(x, GlobalMaxPooling1D()(z_mean))\n",
    "            \n",
    "        def kl_loss(X, _X):\n",
    "\n",
    "        #     rec_loss = objectives.sparse_categorical_crossentropy(X, _X)\n",
    "            kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "            return kl_loss\n",
    "\n",
    "        self.model.compile(optimizer=\"adam\", loss=[\"sparse_categorical_crossentropy\", kl_loss])\n",
    "        \n",
    "    def name(self):\n",
    "        \n",
    "        return \"nvdm%d\" % self.mode\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to BOW\n",
    "# X_q_july = np.zeros((len(q_july), nb_words))\n",
    "# for idx, i in enumerate(q_july):\n",
    "#     X_q_july[idx, i] = 1\n",
    "# x_q_july = q_july\n",
    "# x_q_july = np.expand_dims(x_q_july, axis=-1)\n",
    "\n",
    "# X_d_july = np.zeros((len(d_july), nb_words))\n",
    "# for idx, i in enumerate(d_july):\n",
    "#     X_d_july[idx, i] = 1\n",
    "# x_d_july = d_july\n",
    "# x_d_july = np.expand_dims(x_d_july, axis=-1)\n",
    "\n",
    "# X_df = np.zeros((len(q_df), nb_words))\n",
    "# for idx, i in enumerate(q_df):\n",
    "#     X_df[idx, i] = 1\n",
    "# x_df = q_df\n",
    "# y_df = np.expand_dims(x_df, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval(epoch, logs):\n",
    "    \n",
    "    loss = logs.get(\"loss\")\n",
    "    val_loss = logs.get(\"val_loss\")\n",
    "    \n",
    "#     may_ndcg, june_ndcg, july_auc = evaluate(run, cosine, test_set)\n",
    "#     print_output = '%s, Epoch %d, [%.1f s], May = %.4f, June = %.4f, July = %.4f, Loss = %.4f, V_Loss = %.4f \\n' % (run.name(), epoch, 0, may_ndcg, june_ndcg, july_auc, loss, val_loss)\n",
    "#     print(print_output)\n",
    "    \n",
    "#     generate_output(run.model.predict(q_july), run.model.predict(d_july))\n",
    "\n",
    "    if hasattr(run, 'generator'):\n",
    "        generate_output(run.generator.predict(q_july), run.generator.predict(d_july))\n",
    "    elif \"seqvae\" in run.name():\n",
    "\n",
    "        generate_output(run.model.predict([q_july, _q_july]), run.model.predict([d_july, _d_july]))\n",
    "    elif \"kate_lstm\" in run.name() or \"vae_lstm\" in run.name():\n",
    "#         sample new query\n",
    "#         generate_output(run.model.predict(q_july)[0], run.model.predict(d_july)[0])\n",
    "        generate_output(run.decoder.predict(run.encoder.predict(q_july)), run.decoder.predict(run.encoder.predict(d_july)))\n",
    "    elif \"naive_vae\" in run.name() or \"nvdm\" in run.name():\n",
    "        print(\"no generated query\")\n",
    "    else:\n",
    "        generate_output(run.model.predict(q_july), run.model.predict(d_july))\n",
    "    \n",
    "    \n",
    "    #       combine with w2v\n",
    "    if run.name() == \"nvdm0\":\n",
    "        q = run.encoder.predict(X_q_july)\n",
    "        d = run.encoder.predict(X_d_july)\n",
    "    else:\n",
    "        q = run.encoder.predict(q_july)\n",
    "        d = run.encoder.predict(d_july)\n",
    "#     q = np.concatenate([w2v.predict(q_july), run.encoder.predict(q_july)], axis=-1)\n",
    "#     d = np.concatenate([w2v.predict(d_july), run.encoder.predict(d_july)], axis=-1)\n",
    "    \n",
    "    cosine = CosineSim(q.shape[-1])\n",
    "    \n",
    "    pred = cosine.model.predict([q, d])\n",
    "    print(loss, val_loss, auc(qrel_july, pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE_LSTM(object):\n",
    "    \n",
    "    def __init__(self, nb_words, max_len, emb, dim, comp_topk=None, ctype=None, epsilon_std=1.0, optimizer=Adadelta(lr=2.), enableGAN=False, kl_weight=0):\n",
    "        self.dim = dim\n",
    "        self.comp_topk = comp_topk\n",
    "        self.ctype = ctype\n",
    "        self.epsilon_std = epsilon_std\n",
    "\n",
    "        self.nb_words = nb_words\n",
    "        self.max_len = max_len\n",
    "        self.emb = emb\n",
    "        self.enableGAN = enableGAN\n",
    "        self.optimizer = optimizer\n",
    "        self.kl_weight = kl_weight\n",
    "        \n",
    "        act = 'tanh'\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        input_layer = Input(shape=(self.max_len,))\n",
    "        mask = Masking(mask_value=0)\n",
    "        self.embed_layer = emb\n",
    "#         self.embed_layer = Embedding(nb_words, 200)\n",
    "#         self.embed_layer.mask_zero = True\n",
    "        # self.embed_layer.trainable = False\n",
    "        bilstm = Bidirectional(LSTM(self.dim[0], return_sequences=True))\n",
    "#         bilstm = LSTM(self.dim[0], return_sequences=True)\n",
    "\n",
    "\n",
    "        hidden_layer1 = Dense(self.dim[0], kernel_initializer='glorot_normal', activation=act)\n",
    "        \n",
    "        h1 = self.embed_layer(mask(input_layer))\n",
    "#         h1 = self.embed_layer(input_layer)\n",
    "\n",
    "        h1 = bilstm(h1)\n",
    "#         h1 = hidden_layer1(h1)\n",
    "\n",
    "        self.z_mean = Dense(self.dim[1], kernel_initializer='glorot_normal')(h1)\n",
    "        self.z_log_var = Dense(self.dim[1], kernel_initializer='glorot_normal')(h1)\n",
    "\n",
    "        if self.comp_topk != None:\n",
    "            self.z_mean_k = KCompetitive(self.comp_topk, self.ctype)(self.z_mean)\n",
    "            encoded = Lambda(self.sampling, output_shape=(self.max_len, self.dim[1],))([self.z_mean_k, self.z_log_var])\n",
    "        else:\n",
    "            encoded = Lambda(self.sampling, output_shape=(self.max_len,self.dim[1],))([self.z_mean, self.z_log_var])\n",
    "\n",
    "\n",
    "        # we instantiate these layers separately so as to reuse them later\n",
    "#         decoder_h = Dense(self.dim[0], kernel_initializer='glorot_normal', activation=act)\n",
    "        # decoder_mean = Dense_tied(self.nb_words, activation='softmax', tied_to=hidden_layer1)\n",
    "        decoder_mean = Dense(self.nb_words, activation='softmax')\n",
    "\n",
    "#         h_decoded = decoder_h(encoded)\n",
    "#         h_decoded = RepeatVector(self.max_len)(h_decoded)\n",
    "#         h_decoded = LSTM(self.dim[0], return_sequences=True, unroll=True)(encoded)    \n",
    "        dec_lstm = Bidirectional(LSTM(self.dim[0], return_sequences=True))\n",
    "    \n",
    "        dec_hidden = TimeDistributed(decoder_mean, name='decoded_mean')\n",
    "    \n",
    "        h_decoded = dec_lstm(encoded)\n",
    "        x_decoded_mean = dec_hidden(h_decoded)\n",
    "\n",
    "\n",
    "        self.model = Model(outputs=[x_decoded_mean, x_decoded_mean], inputs=input_layer)\n",
    "\n",
    "        self.encoder1 = Model(outputs=GlobalMaxPooling1D()(encoded), inputs=input_layer)\n",
    "        self.encoder2 = Model(outputs=GlobalAveragePooling1D()(encoded), inputs=input_layer)\n",
    "        self.encoder3 = Model(outputs=GlobalMaxPooling1D()(self.z_mean), inputs=input_layer)\n",
    "        self.encoder4 = Model(outputs=GlobalAveragePooling1D()(self.z_mean), inputs=input_layer)\n",
    "\n",
    "\n",
    "        # build a digit generator that can sample from the learned distribution\n",
    "        decoder_input = Input(shape=(self.max_len, self.dim[1],))\n",
    "        _h_decoded = dec_lstm(decoder_input)\n",
    "        _x_decoded_mean = dec_hidden(_h_decoded)\n",
    "#         _h_decoded = RepeatVector(self.max_len)(decoder_input)\n",
    "#         _h_decoded = Bidirectional(LSTM(self.dim[0], return_sequences=True, unroll=True))(_h_decoded)\n",
    "#         _x_decoded_mean = TimeDistributed(decoder_mean, name='decoded_mean')(_h_decoded)\n",
    "\n",
    "        self.decoder = Model(outputs=_x_decoded_mean, inputs=decoder_input)\n",
    "\n",
    "\n",
    "\n",
    "        if enableGAN:\n",
    "            self.model.compile(optimizer=self.optimizer, loss=[self.vae_loss, \"binary_crossentropy\"])\n",
    "        else:    \n",
    "            self.model.compile(optimizer=self.optimizer, loss=[\"sparse_categorical_crossentropy\", self.kl_loss])\n",
    "\n",
    "    def name(self):\n",
    "        return \"kate_lstm_k%d\" % self.comp_topk if self.ctype != None else \"vae_lstm\"\n",
    "    def kl_loss(self, x, x_):\n",
    "\n",
    "        kl_loss = - 0.5 * K.sum(1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var), axis=-1)\n",
    "\n",
    "        return self.kl_weight * kl_loss\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.max_len, self.dim[1]), mean=0.,\\\n",
    "                                  stddev=self.epsilon_std)\n",
    "\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run = VAE_LSTM(nb_words, max_len, bpe.get_keras_embedding(True), [300,128], optimizer=RMSprop(), kl_weight=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run.model.compile(optimizer=RMSprop(), loss=[\"sparse_categorical_crossentropy\", run.kl_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 3s - loss: 9.4979 - decoded_mean_loss_1: 9.4939 - decoded_mean_loss_2: 0.0040\n",
      "\n",
      "0.49456292112344064\n",
      "0.46358602592897324\n",
      "0.4621405857368409\n",
      "Epoch 1/1\n",
      " - 1s - loss: 3.9243 - decoded_mean_loss_1: 3.8838 - decoded_mean_loss_2: 0.0404\n",
      "\n",
      "0.5258956169805866\n",
      "0.43873557339500546\n",
      "0.47700638217438673\n",
      "Epoch 1/1\n",
      " - 1s - loss: 3.4642 - decoded_mean_loss_1: 3.4362 - decoded_mean_loss_2: 0.0280\n",
      "\n",
      "0.5021792790589072\n",
      "0.4286508483622051\n",
      "0.4722253107696413\n",
      "Epoch 1/1\n",
      " - 1s - loss: 3.3727 - decoded_mean_loss_1: 3.3501 - decoded_mean_loss_2: 0.0226\n",
      "\n",
      "0.49636416197825173\n",
      "0.4239920834352554\n",
      "0.45942760568391555\n",
      "Epoch 1/1\n",
      " - 1s - loss: 3.2479 - decoded_mean_loss_1: 3.2276 - decoded_mean_loss_2: 0.0203\n",
      "\n",
      "0.48304387466921656\n",
      "0.42187951700060045\n",
      "0.45433520869932614\n",
      "Epoch 1/1\n",
      " - 1s - loss: 3.1581 - decoded_mean_loss_1: 3.1392 - decoded_mean_loss_2: 0.0189\n",
      "\n",
      "0.47285908070003774\n",
      "0.42778358424692564\n",
      "0.459105161333363\n",
      "Epoch 1/1\n",
      " - 1s - loss: 3.1424 - decoded_mean_loss_1: 3.1234 - decoded_mean_loss_2: 0.0190\n",
      "\n",
      "0.49236140452311594\n",
      "0.4267161822588894\n",
      "0.46797794035891394\n",
      "Epoch 1/1\n",
      " - 1s - loss: 3.0094 - decoded_mean_loss_1: 2.9914 - decoded_mean_loss_2: 0.0179\n",
      "\n",
      "0.488558784940737\n",
      "0.42780582178834303\n",
      "0.4685338788943495\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.9483 - decoded_mean_loss_1: 2.9323 - decoded_mean_loss_2: 0.0161\n",
      "\n",
      "0.45326780671129\n",
      "0.43324290066490245\n",
      "0.4734372567768908\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.8548 - decoded_mean_loss_1: 2.8375 - decoded_mean_loss_2: 0.0173\n",
      "\n",
      "0.45600302430563266\n",
      "0.4286953234450399\n",
      "0.46687718205875156\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.7602 - decoded_mean_loss_1: 2.7449 - decoded_mean_loss_2: 0.0153\n",
      "\n",
      "0.4747270341791011\n",
      "0.4336098200982899\n",
      "0.4690231048055327\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.6754 - decoded_mean_loss_1: 2.6590 - decoded_mean_loss_2: 0.0165\n",
      "\n",
      "0.4791300673797505\n",
      "0.4362338499855456\n",
      "0.4688340857034846\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.6320 - decoded_mean_loss_1: 2.6154 - decoded_mean_loss_2: 0.0166\n",
      "\n",
      "0.48335520024906053\n",
      "0.4402366074406814\n",
      "0.48353310058039983\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.5068 - decoded_mean_loss_1: 2.4890 - decoded_mean_loss_2: 0.0178\n",
      "\n",
      "0.48400008895016566\n",
      "0.4419711356712402\n",
      "0.47414885810224827\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.4188 - decoded_mean_loss_1: 2.4019 - decoded_mean_loss_2: 0.0169\n",
      "\n",
      "0.4693233116146679\n",
      "0.44114834663879565\n",
      "0.473503969401143\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.3446 - decoded_mean_loss_1: 2.3268 - decoded_mean_loss_2: 0.0177\n",
      "\n",
      "0.48863661633569794\n",
      "0.4467522070759857\n",
      "0.46864506660143657\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.2636 - decoded_mean_loss_1: 2.2460 - decoded_mean_loss_2: 0.0177\n",
      "\n",
      "0.49113833974515775\n",
      "0.4467966821588205\n",
      "0.4649202784140186\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.1749 - decoded_mean_loss_1: 2.1569 - decoded_mean_loss_2: 0.0181\n",
      "\n",
      "0.48400008895016566\n",
      "0.4462629811648024\n",
      "0.4673219328871\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.0960 - decoded_mean_loss_1: 2.0778 - decoded_mean_loss_2: 0.0182\n",
      "\n",
      "0.460294869799195\n",
      "0.45184460406057503\n",
      "0.4617847850741622\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.0108 - decoded_mean_loss_1: 1.9920 - decoded_mean_loss_2: 0.0188\n",
      "\n",
      "0.46663256910316\n",
      "0.45112188396450886\n",
      "0.455224710356023\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.9382 - decoded_mean_loss_1: 1.9182 - decoded_mean_loss_2: 0.0200\n",
      "\n",
      "0.4712135026351486\n",
      "0.45200026685049705\n",
      "0.4561920434076808\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.8809 - decoded_mean_loss_1: 1.8618 - decoded_mean_loss_2: 0.0191\n",
      "\n",
      "0.4984989659543241\n",
      "0.4606284329204563\n",
      "0.4609397585003002\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.7914 - decoded_mean_loss_1: 1.7716 - decoded_mean_loss_2: 0.0198\n",
      "\n",
      "0.4866908314616736\n",
      "0.4579376904089484\n",
      "0.46174030999132737\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.6699 - decoded_mean_loss_1: 1.6490 - decoded_mean_loss_2: 0.0209\n",
      "\n",
      "0.5109742266894972\n",
      "0.45880495452422776\n",
      "0.462829949520781\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.6139 - decoded_mean_loss_1: 1.5934 - decoded_mean_loss_2: 0.0205\n",
      "\n",
      "0.4731259311970469\n",
      "0.45671462563099025\n",
      "0.4712579777179835\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.5408 - decoded_mean_loss_1: 1.5196 - decoded_mean_loss_2: 0.0211\n",
      "\n",
      "0.48435588961284437\n",
      "0.4616513598256577\n",
      "0.46325246280771193\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.4605 - decoded_mean_loss_1: 1.4397 - decoded_mean_loss_2: 0.0209\n",
      "\n",
      "0.4868020191687607\n",
      "0.46563187973937603\n",
      "0.46596544286063735\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.3853 - decoded_mean_loss_1: 1.3635 - decoded_mean_loss_2: 0.0218\n",
      "\n",
      "0.48284373679645987\n",
      "0.4604060575062821\n",
      "0.46276323689652865\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.2727 - decoded_mean_loss_1: 1.2506 - decoded_mean_loss_2: 0.0221\n",
      "\n",
      "0.48835864706798016\n",
      "0.45608085570059376\n",
      "0.46865618537214526\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.2295 - decoded_mean_loss_1: 1.2079 - decoded_mean_loss_2: 0.0216\n",
      "\n",
      "0.4828437367964598\n",
      "0.46729969534568255\n",
      "0.4679890591296227\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.1466 - decoded_mean_loss_1: 1.1245 - decoded_mean_loss_2: 0.0221\n",
      "\n",
      "0.48718005737285686\n",
      "0.46645466877182057\n",
      "0.46705508239009097\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.0768 - decoded_mean_loss_1: 1.0539 - decoded_mean_loss_2: 0.0228\n",
      "\n",
      "0.505103515755298\n",
      "0.46981253752585117\n",
      "0.46950121194600725\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.9810 - decoded_mean_loss_1: 0.9579 - decoded_mean_loss_2: 0.0231\n",
      "\n",
      "0.4818430474326758\n",
      "0.4611176588316396\n",
      "0.4687451355378149\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.9332 - decoded_mean_loss_1: 0.9109 - decoded_mean_loss_2: 0.0224\n",
      "\n",
      "0.49247259223020307\n",
      "0.4664546687718206\n",
      "0.465787542529298\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.8490 - decoded_mean_loss_1: 0.8258 - decoded_mean_loss_2: 0.0232\n",
      "\n",
      "0.4984322533300718\n",
      "0.4719473415019235\n",
      "0.46238519869243255\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.8399 - decoded_mean_loss_1: 0.8165 - decoded_mean_loss_2: 0.0234\n",
      "\n",
      "0.48713558229002196\n",
      "0.46576530498788055\n",
      "0.46169583490849253\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.7325 - decoded_mean_loss_1: 0.7086 - decoded_mean_loss_2: 0.0239\n",
      "\n",
      "0.49078253908247904\n",
      "0.4700571504814428\n",
      "0.4668327069759167\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.6635 - decoded_mean_loss_1: 0.6394 - decoded_mean_loss_2: 0.0242\n",
      "\n",
      "0.5034579376904089\n",
      "0.46420867708866104\n",
      "0.4667882318930819\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.6650 - decoded_mean_loss_1: 0.6418 - decoded_mean_loss_2: 0.0231\n",
      "\n",
      "0.4941848829193444\n",
      "0.4653427917009495\n",
      "0.4638306388845649\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.5443 - decoded_mean_loss_1: 0.5206 - decoded_mean_loss_2: 0.0237\n",
      "\n",
      "0.48766928328404013\n",
      "0.4653205541595321\n",
      "0.4610731837488047\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.5407 - decoded_mean_loss_1: 0.5164 - decoded_mean_loss_2: 0.0243\n",
      "\n",
      "0.4994774177766906\n",
      "0.4708577019724699\n",
      "0.45605861815917637\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.4928 - decoded_mean_loss_1: 0.4683 - decoded_mean_loss_2: 0.0245\n",
      "\n",
      "0.4893593364317641\n",
      "0.46696613222442124\n",
      "0.4581823033645399\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.4383 - decoded_mean_loss_1: 0.4148 - decoded_mean_loss_2: 0.0236\n",
      "\n",
      "0.4893370988903467\n",
      "0.4700126753986079\n",
      "0.45856034156863623\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3751 - decoded_mean_loss_1: 0.3513 - decoded_mean_loss_2: 0.0238\n",
      "\n",
      "0.49307300584847336\n",
      "0.47130245280081834\n",
      "0.45606973692988506\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3870 - decoded_mean_loss_1: 0.3636 - decoded_mean_loss_2: 0.0234\n",
      "\n",
      "0.5049478529653761\n",
      "0.4695234494874247\n",
      "0.4585825791100536\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3213 - decoded_mean_loss_1: 0.2975 - decoded_mean_loss_2: 0.0238\n",
      "\n",
      "0.5073050323556227\n",
      "0.4687228979963975\n",
      "0.45944984322533294\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.2825 - decoded_mean_loss_1: 0.2590 - decoded_mean_loss_2: 0.0235\n",
      "\n",
      "0.4975872267562099\n",
      "0.46787787142253545\n",
      "0.4609397585003002\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.2862 - decoded_mean_loss_1: 0.2631 - decoded_mean_loss_2: 0.0231\n",
      "\n",
      "0.49820987791589755\n",
      "0.46801129667104\n",
      "0.460561720296204\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.2364 - decoded_mean_loss_1: 0.2137 - decoded_mean_loss_2: 0.0227\n",
      "\n",
      "0.4998776935222042\n",
      "0.4659876804020548\n",
      "0.4586270541928884\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.2122 - decoded_mean_loss_1: 0.1894 - decoded_mean_loss_2: 0.0228\n",
      "\n",
      "0.5016344592941805\n",
      "0.464942515955436\n",
      "0.4597611688051768\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    run.model.fit(q_df, [np.expand_dims(q_df, axis=-1), np.expand_dims(q_df, axis=-1)], verbose=2, batch_size=256, epochs=1, callbacks=[TQDMNotebookCallback()])\n",
    "#     generate_output(run.model.predict(q_july)[0], run.model.predict(d_july)[0])\n",
    "    \n",
    "#     generate_output(run.decoder.predict(run.encoder.predict(q_july)), run.deredict(run.encoder.predict(d_july)))\n",
    "    \n",
    "    q1 = run.encoder1.predict(q_july)\n",
    "    d1 = run.encoder1.predict(d_july)\n",
    "    q2 = run.encoder3.predict(q_july)\n",
    "    d2 = run.encoder3.predict(d_july)\n",
    "    q3 = run.encoder4.predict(q_july)\n",
    "    d3 = run.encoder4.predict(d_july)\n",
    "    \n",
    "#     __q = np.concatenate([q1,q2], axis=-1)\n",
    "#     __d = np.concatenate([d1,d2], axis=-1)\n",
    "    __q = q1\n",
    "    __d = d1\n",
    "#     print(__q.shape, __d.shape)\n",
    "    cosine = CosineSim(__q.shape[-1])\n",
    "    pred = cosine.model.predict([__q, __d])\n",
    "    print(auc(qrel_july, pred.flatten()))\n",
    "    \n",
    "    __q = q2\n",
    "    __d = d2\n",
    "#     print(__q.shape, __d.shape)\n",
    "    cosine = CosineSim(__q.shape[-1])\n",
    "    pred = cosine.model.predict([__q, __d])\n",
    "    print(auc(qrel_july, pred.flatten()))\n",
    "    \n",
    "    __q = q3\n",
    "    __d = d3\n",
    "#     print(__q.shape, __d.shape)\n",
    "    cosine = CosineSim(__q.shape[-1])\n",
    "    pred = cosine.model.predict([__q, __d])\n",
    "    print(auc(qrel_july, pred.flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "00e288757f7b4e278529b3bd71f07291": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "06e65b27baee478e99ae7410fea937c8": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "0839ee371ed44f8da6c78aafef9efacd": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "0b1ee1e729e44b41bf3550b96a241349": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "0b4892988b4d45919dc1b8a9191ac238": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "15535f3a48584560b3b77ab07640d052": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "245f89476e8a4404a4a612a810cd8d01": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "28095ab5d2c14890b690248c52d91439": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "3071c135c5224fe1a47b156847981a55": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "34177660e80e44bfaff43d2c3fb86dc0": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "365af3d4c28c4a7d95b916115e8013e7": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "36b8d2803875480289fc7857f9e0a3ec": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "3802c3c9cc2a4e3da26c7600e9f2eaa1": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "38306d9fde8b414e9bc34a5b0722892d": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "4002e497390f4af9872964d6d7f3dbca": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "4364766591994b7592adddef93f8bbd3": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "43ec8c01a3e84d618e587d479957161f": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "454d0738b7f744c6bae0f6d1e46337fa": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "4b31136a3be74849baf327e4fc680ae3": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "55d446bf11e74f8e8482e1aa08dc0d24": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "575b5f0bfec6420a83a6bad291327d78": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "60a07701b96c4c6399d041bfac24a02c": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "60e6873499d14805a04b8b6321a1a0a8": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "6202a3cfb7574636b32484afc16956b8": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "64aa5790b8d14731a8d9f84dc993c00b": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "69eff61099b849619a23175152e4e2d7": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "6c85031cf84848acb17c17030e76bd7e": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "7146a4adf766432aa38d131bf8e1fc4e": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "7182c1d3911d4407a73971f8ed0ed693": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "73db69bb6bb6487d9b1924ec073f8d48": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "7a654d684a7f4f689ce5b3d8103f7c77": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "8027ffa93e8d45f3a9cbb95036e7d622": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "8282d80daeb5443583da37b770bd233e": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "880636b6d69a43c38678990fe4b9a4f3": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "9cc867d231994a37898c3686bce023d1": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "a10841efeca342c49669e98ebcfdd8ef": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "a2f3633c1cd94ac6baecaa74ca06fbb7": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "ae3c7a472b9f477c8ff86e7205bf1496": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "af3f330f22ab4befacc2bb6afbc5d61f": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "b7072de72ff74142bd785d9be598136d": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "b7cee9c7387b4055a1bc1058f6b3115f": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "d0727394d00e471baff81ee0c57b225c": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "d80ebe1f95294660bbcaee0fe70ceafa": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "de740490ba0d4f6db3fbff6b7d63111e": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "deae642fb0cb44eb8f2e615ea5599660": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "e7923efd8fbd4d1db611dd1c28b6f22a": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "f2cd4803d5d647799863aedb9e47381a": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "fc71baaf7f7f4854b9f854f8649205aa": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "fd95c93e119041f08ee90be2db007bc7": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    },
    "fe7d4b97a05b4cb0bf28d615fa865ef9": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
