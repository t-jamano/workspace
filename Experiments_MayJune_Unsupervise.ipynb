{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Models import *\n",
    "from Utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pre-trained trigram\n",
    "tokeniser = L3wTransformer()\n",
    "tokeniser = tokeniser.load(\"/work/data/trigram/%s\" % \"2M_50k_trigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49881, 0, 2095, 1972, 36, 6450, 6450, 49881, 0]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('/work/data/bpe/en.wiki.bpe.op50000.model')\n",
    "s.EncodeAsIds(\"I love you ddddd 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/work/data/bpe/en.wiki.bpe.op50000.model')\n",
    "text = \"test this one\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = []\n",
    "for text in df_july.q.tolist():\n",
    "    q.append([model.index2word.index(t) if t in model.index2word else model.index2word.index('<unk>') for t in sp.EncodeAsPieces(text)])\n",
    "q = pad_sequences(q, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "for text in df_july.d.tolist():\n",
    "    d.append([model.index2word.index(t) if t in model.index2word else model.index2word.index('<unk>') for t in sp.EncodeAsPieces(text)])\n",
    "\n",
    "d = pad_sequences(d, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "w2v = Sequential()\n",
    "w2v.add(model.get_keras_embedding(train_embeddings=False))\n",
    "w2v.add(GlobalAveragePooling1D())\n",
    "\n",
    "pred = get_cosine_sim(w2v.predict(q), w2v.predict(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"/work/data/bpe/en.wiki.bpe.op50000.d200.w2v.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine = CosineSim(feature_num=nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5060819675776647"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cosine.model.predict([q_july, d_july]).flatten()\n",
    "auc(qrel_july, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.795042\n",
      "MAP: 0.690643\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = cosine.model.predict([q_test, d_test])\n",
    "pred = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, isQrel=False)\n",
    "evaluate(qrel, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(texts)\n",
    "# sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "def get_pretrain_w2v(tokeniser, nb_words):\n",
    "    \n",
    "    with open(r\"/data/t-mipha/data/agi_encoder/v4/universal/embedding_dict.pkl\", \"rb\") as input_file:\n",
    "        embeddings_index = pickle.load(input_file)\n",
    "    \n",
    "    \n",
    "    emb_dim = len(embeddings_index['test'])\n",
    "    \n",
    "    word_index = tokeniser.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    nb_words = min(nb_words, len(word_index) + 1)\n",
    "    \n",
    "    embedding_matrix = np.zeros((nb_words, emb_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix, nb_words, emb_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 382057 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, nb_words, emb_dim = get_pretrain_w2v(tokeniser, nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.get_keras_embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.517653\n",
      "MAP: 0.500053\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(nb_words,\n",
    "                            emb_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False)\n",
    "\n",
    "w2v = Sequential()\n",
    "w2v.add(embedding_layer)\n",
    "w2v.add(GlobalAveragePooling1D())\n",
    "\n",
    "pred = get_cosine_sim(w2v.predict(q_test), w2v.predict(d_test))\n",
    "pred = convert_2_trec(df_test.q.tolist(), df_test.d.tolist(), pred, False)\n",
    "evaluate(qrel, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE_DSSM():\n",
    "\n",
    "    def __init__(self, hidden_dim=300, latent_dim=128, nb_words=50005, activation=\"relu\", emb=None):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.nb_words = nb_words\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "        x = Input(shape=(self.nb_words,))\n",
    "        \n",
    "        emb = emb\n",
    "        \n",
    "        lstm = LSTM(self.hidden_dim)\n",
    "\n",
    "        emb_x = lstm(emb(x))\n",
    "\n",
    "        vae_loss, encoded = self.build_encoder(emb_x)\n",
    "        self.encoder = Model(x, encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(self.latent_dim,))\n",
    "\n",
    "        decoded = self.build_decoder(encoded_input)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.model = Model(x, self.build_decoder(encoded))\n",
    "\n",
    "        self.model.compile(optimizer='Adam',\n",
    "                                 loss=vae_loss)\n",
    "        \n",
    "    def build_encoder(self, z):\n",
    "\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            latent_dim = K.shape(z_mean_)[1]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean', activation='linear')(z)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var', activation='linear')(z)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(self.latent_dim,), name='lambda')([z_mean, z_log_var]))\n",
    "    \n",
    "    def build_decoder(self, encoded):\n",
    "\n",
    "        h = Dense(self.hidden_dim, activation=self.activation)(encoded)\n",
    "        decoded = Dense(self.nb_words, activation='sigmoid', name='decoded_mean')(h)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"vae_dssm_h%d_l%d_w%d_%s\" % (self.hidden_dim, self.latent_dim, self.nb_words, self.activation)\n",
    "\n",
    "\n",
    "    def batch_generator(self, reader, train_data, tokeniser, batch_size, max_len, nb_words):\n",
    "        while True:\n",
    "            for df in reader:\n",
    "                q = df.q.tolist()\n",
    "                if train_data == \"1M_EN_QQ_log\":\n",
    "                    d = [i.split(\"<sep>\")[0] for i in df.d.tolist()]\n",
    "                else:\n",
    "                    d = df.d.tolist()\n",
    "                \n",
    "                q = pad_sequences(tokeniser.texts_to_sequences(q), maxlen=max_len)\n",
    "                \n",
    "                q_one_hot = np.zeros((batch_size, nb_words))\n",
    "                for i in range(len(q)):\n",
    "                    q_one_hot[i][q[i]] = 1\n",
    "                    \n",
    "                \n",
    "                yield q_one_hot, q_one_hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
